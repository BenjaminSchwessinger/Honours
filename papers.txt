H, Chan CS, Wilkin P, Remagnino P (2015) Deep-plant: Plant identification with convolutional neural networks. In 2015 IEEE International Conference on Image Processing (ICIP), pp 452-456. 

Paper focuses on the use of convolutional neural networks (CNN) to identify plants from photos of leaf samples. This is typically computationally challenging due to similar shape and colour representation of leaves in nature. They employ a deep learning CNN model to learn leaf features, and also use a deconvolutional network (DN) to visualise the learned features, to avoid using the CNN as a black box solution. 

CNN: used a pre-trained network in part due to small training dataset size. Performance of a CNN is highly dependent on the size, diversity and quality of the training dataset, and training a new model requires skill and time. Fine-tuning was performed using a 44-classes leaf dataset, thus the final fully connected layer consists of 44 neurons replacing the original 1000 neurons. 
DN: To understand why and how the CNN model operates, filter visualisation was used to observe the transformation of features and identify unique features on leaf images important to characterisation. Feature maps from a layer can be deconvolved and unpooled down to the input pixel space using an equation (specified in paper). V1 visualisation examines the highest activation parts across feature maps for each layer to produce an image. 

To enlarge the dataset, rotations of each leaf were produced across multiple orientations, and ~19% of this dataset was set aside for testing, while the remainder was destined for AI training. This D1 dataset included whole-leaf images, and resulted in a CNN with 97.7% identification accuracy. Most misclassified images were done so on the basis of outline shape. 
A second dataset D2 consisted of manually cropped D1 leaf images to patches of leaf such that leaf shape is not a factor, and increasing dataset size by rotation again. This resulted in 99.5% classification accuracy. Visualisation using V1 showed venation as the primary classification mechanism within the CNN. Misclassification in the D2 dataset was determined to primarily be due to the condition of the leaves due to environmental effects, such as wrinkled surface and damage from insects. 

The authors concluded the pairing of a DN-visualisation technique for CNN analysis of images enables better identification of the important features for differentiation, which can allow more targeted imaging of leaves for use in the identification process. 









Boža V, Brejová B, Vinař T (2017) DeepNano: Deep recurrent neural networks for base calling in MinION nanopore reads. PLOS ONE 12: e0178751

The paper presents an open-source DNA base caller, DeepNano, designed for use on the ONT MinION. The base caller is based on deep recurrent neural networks, and shows that base calling can be improved by modern machine learning methods. 

On one version of the MinION, R7.3, the error rate of the reads can be reduced from up to 30% using template reads to ~13-15% for 2D reads (using the Metrichor software on E. coli). This process uses a hidden Markov model with k=6 consecutive bases, where each state represents one k-tuple and the transitions between states correspond to k-tuples overlapping by k-1 bases. Additional transitions can represent missed events and other errors, eg. the indels common to MinION reads. While HMMs are good at representing short-range dependencies, long-range dependencies may play a role in MinION base calling, and unknown DNA sequences may be more difficult on an HMM. 

Recurrent Neural Networks: A type of artificial neural network used for sequence labeling. A sequence of input vectors results in a sequence of output vectors (labels). In this case, input vectors are the mean, standard deviation and length of each event, the output vector gives a probability distribution of bases. Simple RNNs use one hidden layer, moving through a single hidden state to calculate the output vectors. Use of additional layers improves prediction accuracy, but increases computational power and time requirements. Bidirectional RNNs can scan data in both directions, as the prediction for an input vector can be influenced by the data preceding or following it, thus hidden outputs are concatenated before proceeding to the next layer. 

Training: Given a scenario where the correct DNA bases are known for each input event, the goal is to find the parameters of the network that maximise the log likelihood of the correct outputs. 

Results: DeepNano accuracy on 1D strands was significantly better than Metrichor on both strands and in both data sets, however it was only slightly higher in the 2D base calling case. It provides a more accurate GC content than Metrichor, The accuracy achieved extends to the R9 MinION data, where it shares a similar accuracy to the Nanonet RNN-based tool from ONT. 






Wick RR, Judd LM, Holt KE (2019) Performance of neural network basecalling tools for Oxford Nanopore sequencing. Genome Biology 20: 129

This paper examines four base calling programs developed by ONT - Albacore, Guppy, Scrappie and Flappie - alongside R9.4 reads. Albacore is a now discontinued general-purpose CPU base caller, while Guppy is similar to Albacore but running on GPUs for improved base calling speed. Both are only available to ONT customers via their community site. Scrappie is an open-source base caller as a ‘technology demonstrator’ and has recently been replaced by Flappie, which uses a CTC decoder to assign bases. Finally, a 3rd party program, Chiron, was tested. Chiron uses a deeper neural network than ONT’s base callers. Older basecallers like Nanonet and DeepNano were excluded due to no longer being in development. 

Sloika (ONT’s neural network training toolkit) was used to train a model. Read signals were trimmed at the fast5 level, and reads outside the expected size range were discarded after basecalling and quality analysis. Read accuracy was determined by alignment of each base called read set to a reference genome using minimap2, with identity defined as the ’BLAST identity’, and although this overestimates identity (for reads with non-aligned regions) few of these should be present due to the read selection process. 

Guppy superseded Albacore in terms of consensus accuracy with its latest release, although in doing so it sacrificed individual read qscore for the default method. However, it has multiple other modes, which improve both read accuracies but reduce the base calling speed. Scrappie was the worst-performing base caller tested, suggested to be due to an outmoded pre-processing step. It’s successor Flappie shows improvement in read accuracy but not consensus accuracy. In terms of accuracy, Albacore and Guppy are quite similar,  but Guppy drastically outperforms in terms of speed - by more than an order of magnitude. 

















Zou J, Huss M, Abid A, Mohammadi P, Torkamani A, Telenti A (2019) A primer on deep learning in genomics. Nature Genetics 51: 12-18

Machine learning tasks fall within two categories: supervised, where the classification/regression of each data point is predicted using the provided labelled training data set, whereas in unsupervised learning, the goal is learning patterns inherent to the data. The training dataset is used for learning the model parameters, the validation set is used to select the best model, and the test set is kept aside to estimate performance. Deep learning networks have high capacity and are flexible, and so can automatically learn features/patterns with less expert handcrafting. 

