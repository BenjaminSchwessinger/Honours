{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../alignment_to_database.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../alignment_to_database.py\n",
    "\n",
    "\"\"\"\n",
    "Script aims to read in the reference_dataframe file, select a taxonomic\n",
    "level and group, and read the path to the location of that data. It then\n",
    "prepares data for machine learning by converting base pair coding to numerical\n",
    "encoding, pads it out and then runs the algorithm\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "Script aims to read in the reference_dataframe file, and for each\n",
    "sample, randomly subsample n_reads number of reads and save the\n",
    "file, and save the keys used as as separate file. The script ends there.\n",
    "\n",
    "Outside the script these files will be utilised in minimap2 to\n",
    "map against a specified database, produce an output paf file\n",
    "and then remove the file to conserve space. \n",
    "\"\"\")\n",
    "parser.add_argument(\"ref_df_fn\", help=\"File path to the reference dataframe\")\n",
    "parser.add_argument(\"data_root\", help=\"Root folder for analysis/\")\n",
    "parser.add_argument(\"--n_reads\", \"-c\", help=\"count of reads per class\")\n",
    "parser.add_argument(\"--d_type\", \"-d\", help=\"type of database - CUSTOM or UNITE\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# assign required arguments to variables\n",
    "ref_df_fn = args.ref_df_fn\n",
    "data_root = args.data_root\n",
    "\n",
    "# assign a number of reads per class\n",
    "n_reads = int(args.n_reads)\n",
    "\n",
    "# test to make sure both required file paths are input\n",
    "try:\n",
    "    os.path.exists(ref_df_fn)\n",
    "except:\n",
    "    print('Cannot find %s' % ref_df_fn)\n",
    "try:\n",
    "    os.path.exists(data_root)\n",
    "except:\n",
    "    print('Cannot find %s' % data_root)\n",
    "    \n",
    "if args.verbose:\n",
    "    print('\\033[1;34m' + \"Reference dataframe is at \" + ref_df_fn + '\\033[0m')\n",
    "    print('\\033[1;34m' + \"Root directory is at \" + data_root + '\\033[0m')\n",
    "    print('\\033[1;34m' + \"Count of reads per sample is\", n_reads,'\\033[0m')\n",
    "    \n",
    "# read in the reference dataframe from the argument path\n",
    "ref_df = pd.read_csv(ref_df_fn, index_col=None)\n",
    "\n",
    "# check whether the reference dataframe implies there are enough reads\n",
    "# to continue given n_reads\n",
    "try:\n",
    "    if ref_df[ref_df[\"# for use\"] \\\n",
    "              < n_reads].shape[0] > 0 :\n",
    "        print(\"These species need more reads.\")\n",
    "        print(ref_df[ref_df[\"# for use\"] \\\n",
    "              < n_reads])\n",
    "        exit()\n",
    "except:\n",
    "    print('Check %s to have the wanted column names' % ref_df_fn)\n",
    "   \n",
    "\n",
    "for index, row in ref_df.iterrows():\n",
    "    fasta_dict = SeqIO.to_dict(SeqIO.parse(row['path for use'], \"fasta\"))\n",
    "    key_list = []\n",
    "    for key in fasta_dict:\n",
    "        key_list.append(key)\n",
    "    print(len(key_list))\n",
    "    keys_list = random.sample(key_list,k=n_reads)\n",
    "    new_dict = {}\n",
    "    for key in keys_list:\n",
    "        new_dict[key] = fasta_dict[key]\n",
    "    if args.d_type.upper() == 'CUSTOM':\n",
    "        SeqIO.write(new_dict.values(), data_root+\"database_mapping/custom/%s_%s.fasta\" % (row['genus'],row['species']), \"fasta\")\n",
    "        with open(data_root+'database_mapping/custom/%s_%s_keys.csv'% (row['genus'],row['species']), 'w+') as f:\n",
    "            for key in keys_list:\n",
    "                f.write(\"%s\\n\"%(key))\n",
    "    elif args.d_type.upper() == 'UNITE':\n",
    "        SeqIO.write(new_dict.values(), \"analysis/database_mapping/unite/%s_%s.fasta\" % (row['genus'],row['species']), \"fasta\")\n",
    "        with open(data_root+'database_mapping/unite/%s_%s_keys.csv'% (row['genus'],row['species']), 'w+') as f:\n",
    "            for key in keys_list:\n",
    "                f.write(\"%s\\n\"%(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../custom_minimap_result.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../custom_minimap_result.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program extracts a specified number of reads from a fasta\n",
    "file and saves them to a new fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "f=open(args.input_file,\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()\n",
    "\n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp_dict = {}\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[line.split(\",\")[0]] = str(line.split(\",\")[5])\n",
    "\n",
    "count_dict = {}\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for item in tmp_dict:\n",
    "    if tmp_dict[item] not in count_dict:\n",
    "        count_dict[tmp_dict[item]] = 1\n",
    "    else:\n",
    "        count_dict[tmp_dict[item]] = count_dict[tmp_dict[item]] + 1\n",
    "        \n",
    "tmp = pd.DataFrame.from_dict(count_dict,orient='index',columns=[\"Count\"])\n",
    "tmp.index.names = ['analysis/Consensus/'+args.input_file[19:-18]]\n",
    "tmp = tmp.sort_values(by=\"Count\",ascending=False)\n",
    "\n",
    "if args.verbose:\n",
    "    print(tmp)\n",
    "\n",
    "tmp.to_csv(args.input_file[:-4]+'_match_distribution.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
