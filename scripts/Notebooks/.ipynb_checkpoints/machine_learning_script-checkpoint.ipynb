{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../machine_learning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../machine_learning.py\n",
    "\n",
    "\"\"\"\n",
    "Script aims to read in the reference_dataframe file, select a taxonomic\n",
    "level and group, and read the path to the location of that data. It then\n",
    "prepares data for machine learning by converting base pair coding to numerical\n",
    "encoding, pads it out and then runs the algorithm\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def max_seq_len(SeqIO_dict):\n",
    "    \"\"\"\n",
    "    Function takes a SeqIO_dict and returns the lengths of the\n",
    "    longest sequence\n",
    "    \"\"\"\n",
    "    total_lens = []\n",
    "    for key in SeqIO_dict.keys():\n",
    "        total_lens.append(len(SeqIO_dict[key].seq))\n",
    "    return max(total_lens)\n",
    "\n",
    "def numberfy(SeqIO_dict, seq_len, nsubsample):\n",
    "    \"\"\"\n",
    "    Take SeqIO_dict and return SeqIO_dict were bases have been replaced\n",
    "    with numbers\n",
    "    ACGT- replaced with 01234\n",
    "    Take the seq_len each sequence should have\n",
    "    \"\"\"\n",
    "    num_dict = {}\n",
    "    \n",
    "    keys = list(SeqIO_dict.keys())\n",
    "    randkeys = random.sample(keys, k=nsubsample)\n",
    "    \n",
    "    \n",
    "    for key in randkeys:\n",
    "        seq = str(SeqIO_dict[key].seq).replace(\"A\",'0 ')\\\n",
    "        .replace(\"C\",'1 ').replace(\"G\",'2 ').replace(\"T\",'3 ')\n",
    "        seq_new = seq + '4 '*(seq_len -int(len(seq)/2))\n",
    "        num_dict[key] = list(map(int, seq_new.split(' ')[:-1]))\n",
    "    return num_dict\n",
    "\n",
    "def get_model(X_train, Y_train, num_class):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "    return model\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "Script aims to read in the reference_dataframe file, select a taxonomic\n",
    "level and group, and read the path to the location of that data. It then\n",
    "prepares data for machine learning by converting base pair coding to numerical\n",
    "encoding, pads it out and then runs the algorithm\n",
    "\"\"\")\n",
    "parser.add_argument(\"ref_df_fn\", help=\"File path to the reference dataframe\")\n",
    "parser.add_argument(\"data_root\", help=\"Root folder for analysis/\")\n",
    "parser.add_argument(\"--tax_rank\", \"-r\", help=\"taxonomic rank for analysis\")\n",
    "parser.add_argument(\"--name\", \"-n\", help=\"name of rank to select from\")\n",
    "parser.add_argument(\"--n_reads\", \"-c\", help=\"count of reads per class\")\n",
    "parser.add_argument(\"--one\", \"-1\", help=\"first species to test. not used if tax_rank and name present\")\n",
    "parser.add_argument(\"--two\", \"-2\", help=\"second species to test. not used if tax_rank and name present\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# assign required arguments to variables\n",
    "ref_df_fn = args.ref_df_fn\n",
    "data_root = args.data_root\n",
    "\n",
    "# assign a number of reads per class\n",
    "n_reads = int(args.n_reads)\n",
    "\n",
    "# test to make sure both required file paths are input\n",
    "try:\n",
    "    os.path.exists(ref_df_fn)\n",
    "except:\n",
    "    print('Cannot find %s' % ref_df_fn)\n",
    "try:\n",
    "    os.path.exists(data_root)\n",
    "except:\n",
    "    print('Cannot find %s' % data_root)\n",
    "    \n",
    "if args.verbose:\n",
    "    print('\\033[1;34m' + \"Reference dataframe is at \" + ref_df_fn + '\\033[0m')\n",
    "    print('\\033[1;34m' + \"Root directory is at \" + data_root + '\\033[0m')\n",
    "    if args.tax_rank and args.name:\n",
    "        print('\\033[1;34m' + \"Tax Rank is \" + args.tax_rank.lower() + '\\033[0m')\n",
    "        print('\\033[1;34m' + \"Name is \" + args.name.lower() + '\\033[0m')\n",
    "    elif args.one and args.two:\n",
    "        print('\\033[1;34m' + \"Species one is \" + args.one.lower() + '\\033[0m')\n",
    "        print('\\033[1;34m' + \"Species two is \" + args.two.lower() + '\\033[0m')\n",
    "    print('\\033[1;34m' + \"Count of reads per sample is\", n_reads,'\\033[0m')\n",
    "    \n",
    "# read in the reference dataframe from the argument path\n",
    "ref_df = pd.read_csv(ref_df_fn, index_col=None)\n",
    "\n",
    "# check whether the reference dataframe implies there are enough reads\n",
    "# to continue given n_reads\n",
    "try:\n",
    "    if ref_df[ref_df[\"# reads after length filtering\"] \\\n",
    "              < n_reads].shape[0] > 0 :\n",
    "        print(\"These species need more reads.\")\n",
    "        print(ref_df[ref_df[\"# reads after length filtering\"] \\\n",
    "              < n_reads])\n",
    "        #exit()\n",
    "except:\n",
    "    print('Check %s to have the wanted column names' % ref_df_fn)\n",
    "    \n",
    "# assign flagged variables as lower case and assign indices dataframe\n",
    "if args.tax_rank and args.name:\n",
    "    tax_rank = args.tax_rank.lower()\n",
    "    name = args.name.lower()\n",
    "    try:\n",
    "        indices = ref_df[ref_df[tax_rank] == name].index\n",
    "        print(indices)\n",
    "    except:\n",
    "        print(\"Tax_rank or Name not found in reference_dataframe\")\n",
    "        print(tax_rank, name)\n",
    "elif args.one and args.two:\n",
    "    one = args.one.lower()\n",
    "    two = args.two.lower()\n",
    "    try:\n",
    "        indices = ref_df[(ref_df['species'] == one)&(ref_df['species'] == two)].index\n",
    "        print(indices)\n",
    "    except:\n",
    "        print(\"Species inputs not found in reference_dataframe\")\n",
    "        print(one, two)\n",
    "\n",
    "# where the values are that index's path's dataframe\n",
    "SeqIO_dicts = {}\n",
    "for index in indices:\n",
    "    fasta_path = ref_df.loc[index, 'path to length filtering']\n",
    "    try:\n",
    "        SeqIO_dicts[index] = SeqIO.to_dict(SeqIO.parse(fasta_path, \"fasta\"))\n",
    "    except:\n",
    "        print('Check location of fasta files')\n",
    "        print(fasta_path, \"does not exist\")\n",
    "        \n",
    "# each path within an index corresponds to a species\n",
    "# if tax_rank > genus, we want to look at which species are within which genus/family/order etc.\n",
    "\n",
    "# determine the maximum sequence length of accepted sequences\n",
    "total_lens = []\n",
    "for key, value in SeqIO_dicts.items():\n",
    "    total_lens.append(max_seq_len(value))\n",
    "print('\\033[0;32m'+\"The maximum sequence length of all sampled sequences is\"+ '\\033[1;37m',max(total_lens),'\\033[0m')\n",
    "\n",
    "\n",
    "# randomly subsample n_reads number of reads from each index's corresponding\n",
    "# set of reads, convert base pair coding to numerical coding and \n",
    "# pad to the max sequence length\n",
    "numSeqIO_dicts = {}\n",
    "max_len = max(total_lens)\n",
    "if (args.one and args.two) or tax_rank == \"genus\":\n",
    "    for key, value in SeqIO_dicts.items():\n",
    "        numSeqIO_dicts[key] = numberfy(value, max_len, n_reads)\n",
    "else:\n",
    "    location = (ref_df.columns.get_loc(tax_rank)-1)\n",
    "    col_name = ref_df.columns[location]\n",
    "    if args.verbose:\n",
    "        print('location is', col_name)\n",
    "\n",
    "    classes = ref_df.iloc[indices,location].unique()\n",
    "    if args.verbose:\n",
    "        print('classes are', classes)\n",
    "\n",
    "    count_dict = {}\n",
    "    for class_ in classes:\n",
    "        count_dict[class_] = sum(ref_df.iloc[indices,location] == class_)\n",
    "    if args.verbose:\n",
    "        print('count_dict is', count_dict)\n",
    "\n",
    "    min_vals = []\n",
    "    for class_, n_class in count_dict.items():\n",
    "        if n_class == min(count_dict.values()):\n",
    "            min_vals.append(ref_df[ref_df.iloc[:,location] == class_]['# reads after length filtering'].min())\n",
    "    if min(min_vals) % 2 == 0:\n",
    "        minimum_value = min(min_vals)\n",
    "    else:\n",
    "        minimum_value = min(min_vals)-1\n",
    "    if args.verbose:\n",
    "        print('minimum number of reads is', minimum_value)\n",
    "    class_lens_ind = []\n",
    "    if len(count_dict) > 1:\n",
    "        max_reads = 0\n",
    "        for key, value in count_dict.items():\n",
    "            if value == max(count_dict.values()):\n",
    "                max_reads = value*n_reads\n",
    "\n",
    "        if max_reads <= minimum_value:\n",
    "            minimum_value = max_reads\n",
    "\n",
    "        for key, n_class in count_dict.items():\n",
    "            s_reads = int(minimum_value/n_class)\n",
    "            if ref_df[ref_df.loc[:,col_name]==key]['# reads after length filtering'].min() < s_reads:\n",
    "                minimum_value = ref_df[ref_df.loc[:,col_name]==key]['# reads after length filtering'].min()/n_class\n",
    "                s_reads = int(minimum_value/n_class)\n",
    "            if args.verbose:\n",
    "                print('The class is', key, 'and the number of reads to be subsampled is', s_reads)\n",
    "            for keya, value in SeqIO_dicts.items():\n",
    "                if ref_df.loc[keya,col_name] == key:\n",
    "                    numSeqIO_dicts[keya] = numberfy(value, max_len, s_reads)\n",
    "                    class_lens_ind.append(s_reads)\n",
    "        n_reads = minimum_value\n",
    "    elif len(count_dict) == 1:\n",
    "        s_reads = n_reads\n",
    "        print(\"no comparison for the rank\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "location = (ref_df.columns.get_loc(tax_rank)-1)\n",
    "col_name = ref_df.columns[location]\n",
    "classes = ref_df.iloc[indices,location].unique()\n",
    "\n",
    "order = []\n",
    "seq_list = []\n",
    "total_expected_reads = len(classes)*n_reads\n",
    "class_lens = []\n",
    "for class_ in classes:\n",
    "    tmp_sum = []\n",
    "    for key in numSeqIO_dicts.keys():\n",
    "        if ref_df.loc[key,col_name] == class_:\n",
    "            order.append(key)\n",
    "            seq_list.append(np.array(list(numSeqIO_dicts[key].values())))\n",
    "            tmp_sum.append(len(list(numSeqIO_dicts[key].values())))\n",
    "    class_lens.append(sum(tmp_sum))\n",
    "    \n",
    "print(class_lens)\n",
    "if args.verbose:\n",
    "    print(\"Ids order for labels is\", order)\n",
    "    print(\"Number of reads subsampled per id is\", class_lens_ind)\n",
    "    print(\"Total expected reads is\", total_expected_reads)\n",
    "    for i in range(0, len(classes)):\n",
    "        print(classes[i], \"has\", class_lens[i], \"reads\")\n",
    "    print(\"Total reads used is\", sum(class_lens))\n",
    "\n",
    "    \n",
    "seq_comb = np.concatenate(seq_list, axis = 0)\n",
    "\n",
    "# determine the number of classes and generate an array of ids\n",
    "num_class = len(classes)\n",
    "ids_comb = np.zeros( (n_reads*num_class,num_class) )\n",
    "for i in range(0, num_class):\n",
    "    ids_comb[i*n_reads:(i+1)*n_reads,i] = 1\n",
    "\n",
    "print(ids_comb)\n",
    "print(ids_comb.shape)\n",
    "\n",
    "\n",
    "# check to make sure whether the number of reads going in must be\n",
    "# exactly equivalent\n",
    "\n",
    "# currently the code can produce classes with a deviation between\n",
    "# the minimum_value and the total number, especially for large\n",
    "# classes eg. ascomycota vs basidiomycota at the phylum level\n",
    "# minimum_value = 23660\n",
    "# #ascomycota = 23640\n",
    "# #basidiomycota = 23658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
