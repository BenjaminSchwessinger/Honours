{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171103_FAH15473/barcode02/merged.fastq\", \"fastq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for key in fastq_dict:\n",
    "    lengths.append(len(fastq_dict[key].seq))\n",
    "print(\"The number of reads in this file is\", len(fastq_dict))\n",
    "ax = sns.distplot(lengths, color=\"k\", kde=False, bins=5000)\n",
    "\n",
    "\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Reads spread 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA = fastq_dict.copy()\n",
    "for key in fastq_dict:\n",
    "    if len(fastq_dict[key].seq) not in range(2700, 3200):\n",
    "        del frDNA[key]\n",
    "print(\"The number of reads between 2700 and 3200 bp in length is\", len(frDNA))\n",
    "EF1a = fastq_dict.copy()\n",
    "for key in fastq_dict:\n",
    "    if len(fastq_dict[key].seq) not in range(900, 1400):\n",
    "        del EF1a[key]\n",
    "print(\"The number of reads between 900 and 1400 bp in length is\", len(EF1a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the reads defined between the frDNA cutoff into a new fasta file\n",
    "SeqIO.write(frDNA.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_frDNA_clipped.fastq\", \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the reads defined between the EF1a cutoff into a new fasta file\n",
    "SeqIO.write(EF1a.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_EF1a_clipped.fastq\", \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_extract={k: frDNA[k] for k in list(frDNA.keys())[:500]}\n",
    "EF1a_extract={k: EF1a[k] for k in list(EF1a.keys())[:500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in frDNA_extract:\n",
    "    frDNA_extract[key].annotations = 'frDNA'\n",
    "for key in EF1a_extract:\n",
    "    EF1a_extract[key].annotations = 'EF1a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_extract = {}\n",
    "combined_extract.update(frDNA_extract)\n",
    "combined_extract.update(EF1a_extract)\n",
    "print(len(frDNA_extract))\n",
    "print(len(EF1a_extract))\n",
    "print(len(combined_extract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqIO.write(frDNA_extract.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_frDNA_extract_test.fastq\", \"fastq\")\n",
    "SeqIO.write(EF1a_extract.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_EF1a_extract_test.fastq\", \"fastq\")\n",
    "SeqIO.write(combined_extract.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_combined_extract_test.fastq\", \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/frDNA_clipped_test.paf\", sep='\\t', header=None, engine='python')\n",
    "EF1a_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/EF1a_clipped_test.paf\", sep='\\t', header=None, engine='python')\n",
    "combined_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/combined_test.paf\", sep='\\t', header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_paf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_paf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min len of match for frDNA is\", frDNA_paf[1].min())\n",
    "print(\"min len of match for EF1a is\", EF1a_paf[1].min())\n",
    "print(\"min len of match for combined is\", combined_paf[1].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num matches with unique ids for frDNA is', len(frDNA_paf[0].unique()))\n",
    "print('num matches with unique ids for EF1a is', len(EF1a_paf[0].unique()))\n",
    "print('num matches with unique ids for combined is', len(combined_paf[0].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare above (using minimap2) with BLAST approach\n",
    "    - BLAST may be too slow on a larger dataset\n",
    "\n",
    "\n",
    "Check with other alignment programs eg. lastz, BLAT (check for others)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of full size-clipped files for alignment via minimap2\n",
    " - For each of the frDNA_clipped and EF1a_clipped files as created above \n",
    "     - Look for the number of unique ids in the resultant file\n",
    "     - Determine the percentage of reads in this range that match homology given total number of reads in the clipped.fastq file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_clipped_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/frDNA_clipped_test.paf\", sep='\\t', header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EF1a_clipped_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/EF1a_clipped_test.paf\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of matches found for frDNA is', len(frDNA_clipped_paf[0].unique()))\n",
    "print('Percentage of matches in region =', \"{:.3%}\".format((len(frDNA_clipped_paf[0].unique())/167054)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of matches found for EF1a is', len(EF1a_clipped_paf[0].unique()))\n",
    "print('False positive percentage =', \"{:.3%}\".format((len(EF1a_clipped_paf[0].unique())/192712)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of matches found for the total of reads is', len(combined_paf[0].unique()))\n",
    "print('Percentage of matches overall =', \"{:.3%}\".format((len(combined_paf[0].unique())/413127)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Plot distribution of matching reads as above~~\n",
    "\n",
    "~~Repeat minimap2 for above to see if fluctuations~~\n",
    "\n",
    "Explore non-mapping 25%\n",
    "\n",
    "Scaling - plots saved out, file for statistics (loop over later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_ids = []\n",
    "# for key in combined_paf[0].unique():\n",
    "#     combined_ids.append(key)\n",
    "combined_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171103_FAH15473/barcode02/merged.fastq\", \"fastq\"))\n",
    "comb_dict = {}\n",
    "for key in combined_paf[0].unique():\n",
    "    comb_dict[key] = combined_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "comb_keys = []\n",
    "for key in comb_dict:\n",
    "    lengths.append(len(comb_dict[key].seq))\n",
    "    comb_keys.append(key)\n",
    "\n",
    "mean = np.mean(lengths)\n",
    "std = np.std(lengths)\n",
    "print(mean)\n",
    "print(std)\n",
    "    \n",
    "# stats_dict = {'number of frDNA reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "# stats = pd.DataFrame(stats_dict, index=['20171103_FAH15473/barcode02'])\n",
    "        \n",
    "              \n",
    "# ax = sns.distplot(lengths, color=\"k\", kde=False, bins=5000)\n",
    "# ax.set(xlim=(250, 3500))\n",
    "# ax.set_title(\"frDNA reads for 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "# ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ax = sns.distplot(lengths, color=\"k\", kde=False, bins=5000)\n",
    "# ax.set(xlim=(2400, 3500))\n",
    "# ax.set_title(\"frDNA reads for 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "# ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_frDNA_clipped.fastq\", 'fastq'))\n",
    "print(len(fr_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_dict = {}\n",
    "# for key in fr_dict:\n",
    "#     if key not in frDNA_clipped_paf[0].unique():\n",
    "#         non_dict[key] = fr_dict[key]\n",
    "# print(len(non_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = []\n",
    "# non_keys = []\n",
    "# for key in non_dict:\n",
    "#     lengths.append(len(non_dict[key].seq))\n",
    "#     non_keys.append(key)\n",
    "\n",
    "# non_dict_stats = {'number of reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "# stats = pd.DataFrame(non_dict_stats, index=['20171103_FAH15473/barcode02'])\n",
    "        \n",
    "              \n",
    "# ax = sns.distplot(lengths, color=\"k\", kde=False)\n",
    "# # ax.set(xlim=(250, 3500))\n",
    "# ax.set_title(\"non-frDNA reads for 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "# ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# plt.show()\n",
    "# display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../summary_statistics.py\n",
    "\n",
    "\"\"\"\n",
    "The goal of this program is to examine the distribution of reads within\n",
    "each file, and for the files generated after homology analysis.\n",
    "The program will generate summary statistics for the result of the homology\n",
    "analysis and save figures illustrating the read distribution for the\n",
    "frDNA reads\n",
    "\"\"\"\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to examine the distribution of reads within\n",
    "each file, and for the files generated after homology analysis.\n",
    "The program will generate summary statistics for the result of the homology\n",
    "analysis and save figures illustrating the read distribution for the\n",
    "frDNA reads\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"full_file\", help=\"The full, unfiltered file containing all reads for this barcode\")\n",
    "parser.add_argument(\"input_folder\", help=\"The destination folder within which the .paf files are generated\")\n",
    "parser.add_argument(\"output_folder\", help=\"The destination folder for any outputs from this script - including summary statistics file and plots\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('\\033[0;35m'+'START'+'\\033[1;37m')\n",
    "\n",
    "output_folder = args.output_folder.rsplit('/', 1)[-2]\n",
    "input_folder = args.input_folder.rsplit('/', 1)[-2]\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input folder is \" + input_folder + '\\033[1;37m')\n",
    "    print('\\033[0;31m' + \"Output folder is \" + output_folder + '\\033[1;37m')\n",
    "    print('\\033[0;34m' + \"Loading \" + args.full_file + '\\033[1;37m')\n",
    "\n",
    "# Load the full file containing all reads for this barcode\n",
    "full_file_dict = SeqIO.to_dict(SeqIO.parse(args.full_file, \"fastq\"))\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loaded \" + args.full_file + '\\033[1;37m')\n",
    "\n",
    "# Extract the information about the lengths of the sequence for each read in this barcode\n",
    "full_lengths = []\n",
    "for key in full_file_dict:\n",
    "    full_lengths.append(len(full_file_dict[key].seq))\n",
    "full_lengths_len = len(full_file_dict)\n",
    "\n",
    "\n",
    "# Plot the spread of read lengths for this barcode\n",
    "    # Expect to see two peaks - one for EF1a and one for frDNA\n",
    "ax = sns.distplot(full_lengths, color=\"k\", kde=False, bins=5000)\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Read spread for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure1 = ax.get_figure()\n",
    "# Save this figure out\n",
    "figure1.savefig('/'.join([output_folder, 'full_read_spread.png']))\n",
    "figure1.clf()\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Full spread image file saved to \" + '/'.join([output_folder, 'full_read_spread.png']) + '\\033[1;37m')\n",
    "    print('\\033[0;34m' + \"Loading \" + input_folder+\"/combined_test.paf\" + '\\033[1;37m')\n",
    "\n",
    "# Import the PAF file resulting from the minimap2 homology filtering\n",
    "full_paf = pd.read_csv(input_folder+\"/combined_test.paf\", sep='\\t', header=None, engine='python')\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loaded \" + input_folder+\"/combined_test.paf\" + '\\033[1;37m')\n",
    "# Determine all the read ids present within the homology-filtered dataset\n",
    "# Then, create a dictionary extracting all the information from the full read file, but ONLY for reads present within the homology-filtered data\n",
    "full_dict = {}\n",
    "for key in full_paf[0].unique():\n",
    "    full_dict[key] = full_file_dict[key]\n",
    "\n",
    "# For each key in the homology-filtered dictionary, extract the sequence length and key\n",
    "full_paf_lengths = []\n",
    "full_keys = []\n",
    "for key in full_dict:\n",
    "    full_paf_lengths.append(len(full_dict[key].seq))\n",
    "    full_keys.append(key)\n",
    "\n",
    "mean = np.mean(full_paf_lengths)\n",
    "std = np.std(full_paf_lengths)\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[1;33m' + 'Mean read length is %s' % mean + '\\033[1;37m')\n",
    "    print('\\033[1;33m' + 'Standard deviation of read length is %s' % std + '\\033[1;37m')\n",
    "    \n",
    "    \n",
    "length_filt_dict = full_dict.copy()\n",
    "for key in full_keys:\n",
    "    if len(full_dict[key].seq) < (mean-1.645*std) or len(full_dict[key].seq) > (mean+1.645*std):\n",
    "        del length_filt_dict[key]\n",
    "\n",
    "        \n",
    "        \n",
    "SeqIO.write(length_filt_dict.values(), '/'.join([output_folder, 'length_restricted_reads.fasta']), \"fasta\")\n",
    "if args.verbose:\n",
    "    print('\\033[1;36m' + 'Saved %s' % ('/'.join([output_folder, 'length_restricted_reads.fasta'])) + '\\033[1;37m')     \n",
    "    \n",
    "    \n",
    "    \n",
    "length_filt_lens = []\n",
    "len_filt_keys = []\n",
    "for key in length_filt_dict:\n",
    "    length_filt_lens.append(len(length_filt_dict[key].seq))\n",
    "    len_filt_keys.append(key)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Extract the qscores\n",
    "# if args.verbose:\n",
    "#     print('\\033[0;34m' + \"Loading \" + 'Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt' + '\\033[1;37m')\n",
    "# summ_stats_csv = pd.read_csv('Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt', sep='\\t', header=None, names=['filename', 'read_id', 'run_id', 'batch_id', 'channel', 'mux', 'start_time', 'duration', 'num_events', 'passes_filtering', 'template_start', 'num_events_template', 'template_duration', 'sequence_length_template', 'mean_qscore_template', 'strand_score_template', 'median_template', 'mad_template'], engine='python')\n",
    "# summ_stats_csv = pd.DataFrame(summ_stats_csv[1:])\n",
    "# summary_list = []\n",
    "# for column, row in summ_stats_csv.iterrows():\n",
    "#     if row['read_id'] in full_keys:\n",
    "#         summary_list.append([row['read_id'], row['mean_qscore_template']])\n",
    "# summary_frame = pd.DataFrame(summary_list)\n",
    "# if args.verbose:\n",
    "#     print('\\033[0;34m' + \"Finished with \" + 'Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt' + '\\033[1;37m')\n",
    "    \n",
    "# Create a dictionary containing the statistics for the filtered dataset\n",
    "    # Total no. frDNA reads, Min. read length, Max. read length, Mean read length, Median read length, Quality score\n",
    "\n",
    "stats_dict = {'number of frDNA reads':len(length_filt_lens),'minimum read length':min(length_filt_lens),'maximum read length':max(length_filt_lens),'mean read length':\"{:.0f}\".format(np.mean(length_filt_lens)),'std dev':\"{:.0f}\".format(np.std(length_filt_lens)),'median read length':\"{:.0f}\".format(np.median(length_filt_lens))\n",
    "#               ,'min_qscore':\"{:.2f}\".format(min(summary_frame[1].astype(float))), 'max_qscore':\"{:.2f}\".format(max(summary_frame[1].astype(float))), 'mean_qscore':\"{:.2f}\".format(np.mean(summary_frame[1].astype(float))), 'median_qscore':\"{:.2f}\".format(np.median(summary_frame[1].astype(float)))\n",
    "             }\n",
    "stats = pd.DataFrame(stats_dict, index=['%s' % '/'.join(args.full_file.rsplit('/')[-3:-1])])    \n",
    "              \n",
    "bx = sns.distplot(length_filt_lens, color=\"k\", kde=False)\n",
    "bx.set(xlim=(250, 3500))\n",
    "bx.set_title(\"frDNA reads for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "bx.set_xlabel(\"Length of read\", fontsize=13)\n",
    "bx.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure2 = bx.get_figure()\n",
    "figure2.savefig('/'.join([output_folder, 'frDNA_len_filt_full.png']))\n",
    "figure2.clf()\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"frDNA spread image file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_full.png']) + '\\033[1;37m')\n",
    "\n",
    "cx = sns.distplot(length_filt_lens, color=\"k\", kde=False)\n",
    "cx.set(xlim=((mean-1.645*std)-100, (mean+1.645*std)+100))\n",
    "cx.set_title(\"frDNA reads for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "cx.set_xlabel(\"Length of read\", fontsize=13)\n",
    "cx.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure3 = cx.get_figure()\n",
    "figure3.savefig('/'.join([output_folder, 'frDNA_len_filt_limited.png']))\n",
    "figure3.clf()\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Zoomed-in frDNA spread image file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_limited.png']) + '\\033[1;37m')\n",
    "\n",
    "stats.to_csv('/'.join([output_folder, 'frDNA_len_filt_statistics.csv']), index=False)\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Summary statistics file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_statistics.csv']) + '\\033[1;37m')\n",
    "    \n",
    "print('\\033[0;35m'+'END'+'\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Stats/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','standard deviation','median read length','min_qscore','max_qscore','mean_qscore','median_qscore'])\n",
    "for path in path_names:\n",
    "    if path[54:-21] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats = full_stats.append(path_stats_csv)\n",
    "            full_stats = full_stats.rename(index={0: path[36:-21]})\n",
    "full_stats = full_stats.sort_index(ascending=True)\n",
    "full_stats.to_csv('../../analysis/Stats/overall_frDNA_stats.csv')\n",
    "num_read_stats = pd.DataFrame(data=[\"{:.0f}\".format(min(full_stats['number of frDNA reads'])), \"{:.0f}\".format(max(full_stats['number of frDNA reads'])), \"{:.0f}\".format(np.mean(full_stats['number of frDNA reads'])), \"{:.0f}\".format(np.median(full_stats['number of frDNA reads']))], index=['Min', 'Max', 'Mean', 'Median'], columns=['Number of reads'])\n",
    "num_read_stats.to_csv('../../analysis/Stats/number_of_frDNA_reads_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(full_stats['number of frDNA reads']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Length_Filtered/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats2 = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','standard deviation','median read length'])\n",
    "for path in path_names:\n",
    "    if path[64:-30] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats2 = full_stats2.append(path_stats_csv)\n",
    "            full_stats2 = full_stats2.rename(index={0: path[46:-30]})\n",
    "full_stats2 = full_stats2.sort_index(ascending=True)\n",
    "display(full_stats2)\n",
    "full_stats2.to_csv('../../analysis/Length_Filtered/overall_frDNA_stats.csv')\n",
    "num_read_stats = pd.DataFrame(data=[\"{:.0f}\".format(min(full_stats2['number of frDNA reads'])), \"{:.0f}\".format(max(full_stats2['number of frDNA reads'])), \"{:.0f}\".format(np.mean(full_stats2['number of frDNA reads'])), \"{:.0f}\".format(np.median(full_stats2['number of frDNA reads']))], index=['Min', 'Max', 'Mean', 'Median'], columns=['Number of reads'])\n",
    "num_read_stats.to_csv('../../analysis/Length_Filtered/number_of_frDNA_reads_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create matrix for loss-of-reads when performing length filtering as percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stats_1 = []\n",
    "full_stats_2 = []\n",
    "result = []\n",
    "\n",
    "for i in full_stats['number of frDNA reads']:\n",
    "    full_stats_1.append(i)\n",
    "for i in full_stats2['number of frDNA reads']:\n",
    "    full_stats_2.append(i)\n",
    "for i in range (0,len(full_stats_1)):\n",
    "    result.append(float(\"{:.2f}\".format((full_stats_1[i]-full_stats_2[i])/full_stats_1[i])))\n",
    "print(result)\n",
    "print('The maximum loss of reads is %s%% in %s' % (100*max(result), path_names[result.index(max(result))][46:-30]))\n",
    "print('The minimum loss of reads is %s%% in %s' % (100*min(result), path_names[result.index(min(result))][46:-30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAH18688_barcode10 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171212_FAH18688/barcode10/merged.fastq\", 'fastq'))\n",
    "lengths = []\n",
    "b10_keys = []\n",
    "for key in FAH18688_barcode10:\n",
    "    lengths.append(len(FAH18688_barcode10[key].seq))\n",
    "    b10_keys.append(key)\n",
    "\n",
    "non_dict_stats = {'number of reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "stats = pd.DataFrame(non_dict_stats, index=['20171212_FAH18688/barcode10'])\n",
    "              \n",
    "ax = sns.distplot(lengths, color=\"k\", kde=False)\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"non-frDNA reads for 20171212_FAH18688/barcode10\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "plt.show()\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAH18654_barcode10 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171207_FAH18654/barcode10/merged.fastq\", 'fastq'))\n",
    "lengths = []\n",
    "b10_keys = []\n",
    "for key in FAH18654_barcode10:\n",
    "    lengths.append(len(FAH18654_barcode10[key].seq))\n",
    "    b10_keys.append(key)\n",
    "\n",
    "non_dict_stats = {'number of reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "stats = pd.DataFrame(non_dict_stats, index=['20171207_FAH18654/barcode10'])\n",
    "              \n",
    "ax = sns.distplot(lengths, color=\"k\", kde=False)\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"non-frDNA reads for 20171207_FAH18654/barcode10\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "plt.show()\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3807\n",
      "2066\n"
     ]
    }
   ],
   "source": [
    "barcode02 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.fasta\", \"fasta\"))\n",
    "barcode06 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode06/length_restricted_reads.fasta\", \"fasta\"))\n",
    "\n",
    "total_lens = []\n",
    "for key in barcode02:\n",
    "    total_lens.append(len(barcode02[key].seq))\n",
    "for key in barcode06:\n",
    "    total_lens.append(len(barcode06[key].seq))\n",
    "print(max(total_lens))\n",
    "print(min(total_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode02_numbers = {}\n",
    "for key in barcode02:\n",
    "    seq = []\n",
    "    for element in barcode02[key].seq[30:-30]:\n",
    "        if element == \"A\":\n",
    "            seq.append(0)\n",
    "        elif element == \"C\":\n",
    "            seq.append(1)\n",
    "        elif element == \"G\":\n",
    "            seq.append(2)\n",
    "        elif element == \"T\":\n",
    "            seq.append(3)\n",
    "    if len(seq) < max(total_lens):\n",
    "        seq.extend([0]*(max(total_lens)-len(seq)))\n",
    "    barcode02_numbers[key] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2 = np.array(random.choices(list(barcode02_numbers.values()),k=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode06_numbers = {}\n",
    "for key in barcode06:\n",
    "    seq = []\n",
    "    for element in barcode06[key].seq[30:-30]:\n",
    "        if element == \"A\":\n",
    "            seq.append(0)\n",
    "        elif element == \"C\":\n",
    "            seq.append(1)\n",
    "        elif element == \"G\":\n",
    "            seq.append(2)\n",
    "        elif element == \"T\":\n",
    "            seq.append(3)\n",
    "    if len(seq) < max(total_lens):\n",
    "        seq.extend([0]*(max(total_lens)-len(seq)))\n",
    "    barcode06_numbers[key] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq6 = np.array(random.choices(list(barcode06_numbers.values()),k=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_comb = np.concatenate((seq2, seq6), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "ids2 = np.array([2]*(len(seq2)))\n",
    "print(len([2]*(len(seq2))))\n",
    "ids6 = np.array([6]*(len(seq6)))\n",
    "print(len([2]*(len(seq6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_comb = np.concatenate((ids2, ids6), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len(seq_comb))\n",
    "print(len(ids_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids.csv', ids_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [3, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 3, 1, ..., 0, 0, 0],\n",
       "       [2, 3, 3, ..., 0, 0, 0],\n",
       "       [1, 0, 2, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_comb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs.csv', seq_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_test = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs.csv.npz', allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [3, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 3, 1, ..., 0, 0, 0],\n",
       "       [2, 3, 3, ..., 0, 0, 0],\n",
       "       [1, 0, 2, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids.csv.npz', allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_test[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Concatenated/*/*/*.fastq\"\n",
    "path_names = glob.glob(path)\n",
    "total_count = 0\n",
    "can_count = 0\n",
    "mis_count = 0\n",
    "unc_count = 0\n",
    "for path in path_names:\n",
    "    temp_dict = SeqIO.to_dict(SeqIO.parse(path, \"fastq\"))\n",
    "    if path[61:-13] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            total_count += len(temp_dict)\n",
    "            can_count += len(temp_dict)\n",
    "        else:\n",
    "            total_count += len(temp_dict)\n",
    "            mis_count += len(temp_dict)\n",
    "            can_count += len(temp_dict)\n",
    "    else:\n",
    "        total_count += len(temp_dict)\n",
    "        unc_count += len(temp_dict)\n",
    "print('Total number of reads assigned to a barcode by Deepbinner is %s' % can_count)\n",
    "print('Number of reads assigned to non-existent barcodes by Deepbinner is %s' % mis_count)\n",
    "print('Estimated number of misassigned reads in total is %s' % (23*mis_count))\n",
    "print('Percentage of misassigned reads based on estimated number is %s' % (2300*(mis_count)/can_count))\n",
    "print('Number of unclassified reads is %s' % unc_count)\n",
    "print('Percentage of unclassified reads is %s' % (100*unc_count/total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Length_Filtered/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats2 = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','std dev','median read length'])\n",
    "for path in path_names:\n",
    "    if path[64:-30] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats2 = full_stats2.append(path_stats_csv)\n",
    "            full_stats2 = full_stats2.rename(index={0: path[46:-30]})\n",
    "full_stats2 = full_stats2.sort_values('mean read length', ascending=False)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(10,17))\n",
    "ax = sns.barplot(x=full_stats2['mean read length'], y=full_stats2.index, palette=\"Blues_d\", xerr=full_stats2['std dev'],ci=True)\n",
    "ax.set_title(\"Mean Read Length by Sample\")\n",
    "figure1 = ax.get_figure()\n",
    "figure1.savefig(\"/10tb/tmp/TE/honours/analysis/Length_Filtered/mean_reads.png\",bbox_inches = \"tight\")\n",
    "figure1.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Length_Filtered/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats2 = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','std dev','median read length'])\n",
    "for path in path_names:\n",
    "    if path[64:-30] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats2 = full_stats2.append(path_stats_csv)\n",
    "            full_stats2 = full_stats2.rename(index={0: path[46:-30]})\n",
    "full_stats2 = full_stats2.sort_values('number of frDNA reads', ascending=False)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(10,17))\n",
    "ax = sns.barplot(x=full_stats2['number of frDNA reads'], y=full_stats2.index, palette=\"Blues_d\")\n",
    "ax.set_title(\"Number of Reads per Sample after Filtering\")\n",
    "figure1 = ax.get_figure()\n",
    "figure1.savefig(\"/10tb/tmp/TE/honours/analysis/Length_Filtered/num_reads.png\",bbox_inches = \"tight\")\n",
    "figure1.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify primers and orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mothur()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.pcr.seqs(fasta=\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.fasta\",oligos=\"../../analysis/ITS_primers.oligos\",pdiffs=0,rdiffs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.pcr.fasta\",\"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2268\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "\n",
    "for key in pcr_dict:\n",
    "    ids.append(key)\n",
    "with open('../../analysis/Length_Filtered/20171103_FAH15473/barcode02/ids.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.fasta\",\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "keys_list = random.sample(ids,k=200)\n",
    "print(len(keys_list))\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "print(len(tmp_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../get_cons_ids.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../get_cons_ids.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to extract the read ids of reads that\n",
    "contain both the forward and reverse primer as exact matches\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "\n",
    "m = Mothur()\n",
    "m.pcr.seqs(fasta=args.input_file,oligos=args.input_file[:9]+\"ITS_primers.oligos\",pdiffs=0,rdiffs=0)\n",
    "pcr_dict = SeqIO.to_dict(SeqIO.parse(args.input_file[:-5]+\"pcr.fasta\",\"fasta\"))\n",
    "ids = []\n",
    "for key in pcr_dict:\n",
    "    ids.append(key)\n",
    "with open(args.input_file[:-29]+'ids.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)\n",
    "    \n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:-29]+'ids.txt') + '\\033[1;37m')\n",
    "    print('\\033[0;32m' + (\"The number of reads is %s\" % len(ids)) + '\\033[1;37m')\n",
    "    \n",
    "    \n",
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "if len(ids) > 199:\n",
    "    keys_list = random.sample(ids,k=200)\n",
    "elif 100 < len(ids) < 200:\n",
    "    keys_list = random.sample(ids,k=100)\n",
    "else:\n",
    "    print('\\033[1;37m' + \"LOW READS\")\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "SeqIO.write(tmp_dict.values(),(args.input_file[:9]+'Consensus'+args.input_file[24:-29]+'for_consensus.fasta'),'fasta')\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:9]+'Consensus'+args.input_file[24:-29]+'for_consensus.fasta') + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
