{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z. tritici vs C. globuliformis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs0.csv.npz', allow_pickle=True)\n",
    "data = data_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Generate labels dataframe\n",
    "all_labels_onehot = np.zeros( (samples_per_class*2,2) )\n",
    "all_labels_onehot[0:samples_per_class,0] = 1\n",
    "all_labels_onehot[samples_per_class+1:,1] = 1\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print(all_labels_onehot.shape)\n",
    "print(data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_2 = data[:samples_per_class]\n",
    "data_class_6 = data[samples_per_class:]\n",
    "# Print an entry to visualise this\n",
    "print(data_class_2[50])\n",
    "print(data_class_6[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = np.vstack((data_class_2, data_class_6))\n",
    "print('data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=50, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z. tritici vs C. globuliformis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs0.csv.npz', allow_pickle=True)\n",
    "data = data_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Generate labels dataframe\n",
    "all_labels_onehot = np.zeros( (samples_per_class*2,2) )\n",
    "all_labels_onehot[0:samples_per_class,0] = 1\n",
    "all_labels_onehot[samples_per_class+1:,1] = 1\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print(all_labels_onehot.shape)\n",
    "print(data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_2 = data[:samples_per_class]\n",
    "data_class_6 = data[samples_per_class:]\n",
    "# Print an entry to visualise this\n",
    "print(data_class_2[50])\n",
    "print(data_class_6[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = np.vstack((data_class_2, data_class_6))\n",
    "print('data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=50, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z. tritici vs C. globuliformis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs0.csv.npz', allow_pickle=True)\n",
    "data = data_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Generate labels dataframe\n",
    "all_labels_onehot = np.zeros( (samples_per_class*2,2) )\n",
    "all_labels_onehot[0:samples_per_class,0] = 1\n",
    "all_labels_onehot[samples_per_class+1:,1] = 1\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print(all_labels_onehot.shape)\n",
    "print(data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_2 = data[:samples_per_class]\n",
    "data_class_6 = data[samples_per_class:]\n",
    "# Print an entry to visualise this\n",
    "print(data_class_2[50])\n",
    "print(data_class_6[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = np.vstack((data_class_2, data_class_6))\n",
    "print('data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=50, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z. tritici vs C. globuliformis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs0.csv.npz', allow_pickle=True)\n",
    "data = data_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Generate labels dataframe\n",
    "all_labels_onehot = np.zeros( (samples_per_class*2,2) )\n",
    "all_labels_onehot[0:samples_per_class,0] = 1\n",
    "all_labels_onehot[samples_per_class+1:,1] = 1\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print(all_labels_onehot.shape)\n",
    "print(data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_2 = data[:samples_per_class]\n",
    "data_class_6 = data[samples_per_class:]\n",
    "# Print an entry to visualise this\n",
    "print(data_class_2[50])\n",
    "print(data_class_6[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = np.vstack((data_class_2, data_class_6))\n",
    "print('data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=50, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z. tritici vs C. globuliformis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs0.csv.npz', allow_pickle=True)\n",
    "data = data_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Generate labels dataframe\n",
    "all_labels_onehot = np.zeros( (samples_per_class*2,2) )\n",
    "all_labels_onehot[0:samples_per_class,0] = 1\n",
    "all_labels_onehot[samples_per_class+1:,1] = 1\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print(all_labels_onehot.shape)\n",
    "print(data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_2 = data[:samples_per_class]\n",
    "data_class_6 = data[samples_per_class:]\n",
    "# Print an entry to visualise this\n",
    "print(data_class_2[50])\n",
    "print(data_class_6[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = np.vstack((data_class_2, data_class_6))\n",
    "print('data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=50, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
