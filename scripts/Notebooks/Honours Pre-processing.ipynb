{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "\n",
    "_nsre = re.compile('([0-9]+)')\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split(_nsre, s)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171103_FAH15473/barcode02/merged.fastq\", \"fastq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for key in fastq_dict:\n",
    "    lengths.append(len(fastq_dict[key].seq))\n",
    "print(\"The number of reads in this file is\", len(fastq_dict))\n",
    "ax = sns.distplot(lengths, color=\"k\", kde=False, bins=5000)\n",
    "\n",
    "\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Reads spread 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA = fastq_dict.copy()\n",
    "for key in fastq_dict:\n",
    "    if len(fastq_dict[key].seq) not in range(2700, 3200):\n",
    "        del frDNA[key]\n",
    "print(\"The number of reads between 2700 and 3200 bp in length is\", len(frDNA))\n",
    "EF1a = fastq_dict.copy()\n",
    "for key in fastq_dict:\n",
    "    if len(fastq_dict[key].seq) not in range(900, 1400):\n",
    "        del EF1a[key]\n",
    "print(\"The number of reads between 900 and 1400 bp in length is\", len(EF1a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the reads defined between the frDNA cutoff into a new fasta file\n",
    "SeqIO.write(frDNA.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_frDNA_clipped.fastq\", \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the reads defined between the EF1a cutoff into a new fasta file\n",
    "SeqIO.write(EF1a.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_EF1a_clipped.fastq\", \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_extract={k: frDNA[k] for k in list(frDNA.keys())[:500]}\n",
    "EF1a_extract={k: EF1a[k] for k in list(EF1a.keys())[:500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in frDNA_extract:\n",
    "    frDNA_extract[key].annotations = 'frDNA'\n",
    "for key in EF1a_extract:\n",
    "    EF1a_extract[key].annotations = 'EF1a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_extract = {}\n",
    "combined_extract.update(frDNA_extract)\n",
    "combined_extract.update(EF1a_extract)\n",
    "print(len(frDNA_extract))\n",
    "print(len(EF1a_extract))\n",
    "print(len(combined_extract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqIO.write(frDNA_extract.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_frDNA_extract_test.fastq\", \"fastq\")\n",
    "SeqIO.write(EF1a_extract.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_EF1a_extract_test.fastq\", \"fastq\")\n",
    "SeqIO.write(combined_extract.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_combined_extract_test.fastq\", \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/frDNA_clipped_test.paf\", sep='\\t', header=None, engine='python')\n",
    "EF1a_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/EF1a_clipped_test.paf\", sep='\\t', header=None, engine='python')\n",
    "combined_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/combined_test.paf\", sep='\\t', header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_paf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_paf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min len of match for frDNA is\", frDNA_paf[1].min())\n",
    "print(\"min len of match for EF1a is\", EF1a_paf[1].min())\n",
    "print(\"min len of match for combined is\", combined_paf[1].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num matches with unique ids for frDNA is', len(frDNA_paf[0].unique()))\n",
    "print('num matches with unique ids for EF1a is', len(EF1a_paf[0].unique()))\n",
    "print('num matches with unique ids for combined is', len(combined_paf[0].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare above (using minimap2) with BLAST approach\n",
    "    - BLAST may be too slow on a larger dataset\n",
    "\n",
    "\n",
    "Check with other alignment programs eg. lastz, BLAT (check for others)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of full size-clipped files for alignment via minimap2\n",
    " - For each of the frDNA_clipped and EF1a_clipped files as created above \n",
    "     - Look for the number of unique ids in the resultant file\n",
    "     - Determine the percentage of reads in this range that match homology given total number of reads in the clipped.fastq file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_clipped_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/frDNA_clipped_test.paf\", sep='\\t', header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EF1a_clipped_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/EF1a_clipped_test.paf\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of matches found for frDNA is', len(frDNA_clipped_paf[0].unique()))\n",
    "print('Percentage of matches in region =', \"{:.3%}\".format((len(frDNA_clipped_paf[0].unique())/167054)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of matches found for EF1a is', len(EF1a_clipped_paf[0].unique()))\n",
    "print('False positive percentage =', \"{:.3%}\".format((len(EF1a_clipped_paf[0].unique())/192712)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of matches found for the total of reads is', len(combined_paf[0].unique()))\n",
    "print('Percentage of matches overall =', \"{:.3%}\".format((len(combined_paf[0].unique())/413127)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Plot distribution of matching reads as above~~\n",
    "\n",
    "~~Repeat minimap2 for above to see if fluctuations~~\n",
    "\n",
    "Explore non-mapping 25%\n",
    "\n",
    "Scaling - plots saved out, file for statistics (loop over later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_ids = []\n",
    "# for key in combined_paf[0].unique():\n",
    "#     combined_ids.append(key)\n",
    "combined_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171103_FAH15473/barcode02/merged.fastq\", \"fastq\"))\n",
    "comb_dict = {}\n",
    "for key in combined_paf[0].unique():\n",
    "    comb_dict[key] = combined_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "comb_keys = []\n",
    "for key in comb_dict:\n",
    "    lengths.append(len(comb_dict[key].seq))\n",
    "    comb_keys.append(key)\n",
    "\n",
    "mean = np.mean(lengths)\n",
    "std = np.std(lengths)\n",
    "print(mean)\n",
    "print(std)\n",
    "    \n",
    "# stats_dict = {'number of frDNA reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "# stats = pd.DataFrame(stats_dict, index=['20171103_FAH15473/barcode02'])\n",
    "        \n",
    "              \n",
    "# ax = sns.distplot(lengths, color=\"k\", kde=False, bins=5000)\n",
    "# ax.set(xlim=(250, 3500))\n",
    "# ax.set_title(\"frDNA reads for 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "# ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ax = sns.distplot(lengths, color=\"k\", kde=False, bins=5000)\n",
    "# ax.set(xlim=(2400, 3500))\n",
    "# ax.set_title(\"frDNA reads for 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "# ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_frDNA_clipped.fastq\", 'fastq'))\n",
    "print(len(fr_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_dict = {}\n",
    "# for key in fr_dict:\n",
    "#     if key not in frDNA_clipped_paf[0].unique():\n",
    "#         non_dict[key] = fr_dict[key]\n",
    "# print(len(non_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = []\n",
    "# non_keys = []\n",
    "# for key in non_dict:\n",
    "#     lengths.append(len(non_dict[key].seq))\n",
    "#     non_keys.append(key)\n",
    "\n",
    "# non_dict_stats = {'number of reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "# stats = pd.DataFrame(non_dict_stats, index=['20171103_FAH15473/barcode02'])\n",
    "        \n",
    "              \n",
    "# ax = sns.distplot(lengths, color=\"k\", kde=False)\n",
    "# # ax.set(xlim=(250, 3500))\n",
    "# ax.set_title(\"non-frDNA reads for 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "# ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# plt.show()\n",
    "# display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../summary_statistics.py\n",
    "\n",
    "\"\"\"\n",
    "The goal of this program is to examine the distribution of reads within\n",
    "each file, and for the files generated after homology analysis.\n",
    "The program will generate summary statistics for the result of the homology\n",
    "analysis and save figures illustrating the read distribution for the\n",
    "frDNA reads\n",
    "\"\"\"\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to examine the distribution of reads within\n",
    "each file, and for the files generated after homology analysis.\n",
    "The program will generate summary statistics for the result of the homology\n",
    "analysis and save figures illustrating the read distribution for the\n",
    "frDNA reads\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"full_file\", help=\"The full, unfiltered file containing all reads for this barcode\")\n",
    "parser.add_argument(\"input_folder\", help=\"The destination folder within which the .paf files are generated\")\n",
    "parser.add_argument(\"output_folder\", help=\"The destination folder for any outputs from this script - including summary statistics file and plots\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('\\033[0;35m'+'START'+'\\033[1;37m')\n",
    "\n",
    "output_folder = args.output_folder.rsplit('/', 1)[-2]\n",
    "input_folder = args.input_folder.rsplit('/', 1)[-2]\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input folder is \" + input_folder + '\\033[1;37m')\n",
    "    print('\\033[0;31m' + \"Output folder is \" + output_folder + '\\033[1;37m')\n",
    "    print('\\033[0;34m' + \"Loading \" + args.full_file + '\\033[1;37m')\n",
    "\n",
    "# Load the full file containing all reads for this barcode\n",
    "full_file_dict = SeqIO.to_dict(SeqIO.parse(args.full_file, \"fastq\"))\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loaded \" + args.full_file + '\\033[1;37m')\n",
    "\n",
    "# Extract the information about the lengths of the sequence for each read in this barcode\n",
    "full_lengths = []\n",
    "for key in full_file_dict:\n",
    "    full_lengths.append(len(full_file_dict[key].seq))\n",
    "full_lengths_len = len(full_file_dict)\n",
    "\n",
    "\n",
    "# Plot the spread of read lengths for this barcode\n",
    "    # Expect to see two peaks - one for EF1a and one for frDNA\n",
    "ax = sns.distplot(full_lengths, color=\"k\", kde=False, bins=5000)\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Read spread for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure1 = ax.get_figure()\n",
    "# Save this figure out\n",
    "figure1.savefig('/'.join([output_folder, 'full_read_spread.png']))\n",
    "figure1.clf()\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Full spread image file saved to \" + '/'.join([output_folder, 'full_read_spread.png']) + '\\033[1;37m')\n",
    "    print('\\033[0;34m' + \"Loading \" + input_folder+\"/combined_test.paf\" + '\\033[1;37m')\n",
    "\n",
    "# Import the PAF file resulting from the minimap2 homology filtering\n",
    "full_paf = pd.read_csv(input_folder+\"/combined_test.paf\", sep='\\t', header=None, engine='python')\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loaded \" + input_folder+\"/combined_test.paf\" + '\\033[1;37m')\n",
    "# Determine all the read ids present within the homology-filtered dataset\n",
    "# Then, create a dictionary extracting all the information from the full read file, but ONLY for reads present within the homology-filtered data\n",
    "full_dict = {}\n",
    "for key in full_paf[0].unique():\n",
    "    full_dict[key] = full_file_dict[key]\n",
    "\n",
    "# For each key in the homology-filtered dictionary, extract the sequence length and key\n",
    "full_paf_lengths = []\n",
    "full_keys = []\n",
    "for key in full_dict:\n",
    "    full_paf_lengths.append(len(full_dict[key].seq))\n",
    "    full_keys.append(key)\n",
    "\n",
    "mean = np.mean(full_paf_lengths)\n",
    "std = np.std(full_paf_lengths)\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[1;33m' + 'Mean read length is %s' % mean + '\\033[1;37m')\n",
    "    print('\\033[1;33m' + 'Standard deviation of read length is %s' % std + '\\033[1;37m')\n",
    "    \n",
    "    \n",
    "length_filt_dict = full_dict.copy()\n",
    "for key in full_keys:\n",
    "    if len(full_dict[key].seq) < (mean-1.645*std) or len(full_dict[key].seq) > (mean+1.645*std):\n",
    "        del length_filt_dict[key]\n",
    "\n",
    "        \n",
    "        \n",
    "SeqIO.write(length_filt_dict.values(), '/'.join([output_folder, 'length_restricted_reads.fasta']), \"fasta\")\n",
    "if args.verbose:\n",
    "    print('\\033[1;36m' + 'Saved %s' % ('/'.join([output_folder, 'length_restricted_reads.fasta'])) + '\\033[1;37m')     \n",
    "    \n",
    "    \n",
    "    \n",
    "length_filt_lens = []\n",
    "len_filt_keys = []\n",
    "for key in length_filt_dict:\n",
    "    length_filt_lens.append(len(length_filt_dict[key].seq))\n",
    "    len_filt_keys.append(key)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Extract the qscores\n",
    "# if args.verbose:\n",
    "#     print('\\033[0;34m' + \"Loading \" + 'Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt' + '\\033[1;37m')\n",
    "# summ_stats_csv = pd.read_csv('Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt', sep='\\t', header=None, names=['filename', 'read_id', 'run_id', 'batch_id', 'channel', 'mux', 'start_time', 'duration', 'num_events', 'passes_filtering', 'template_start', 'num_events_template', 'template_duration', 'sequence_length_template', 'mean_qscore_template', 'strand_score_template', 'median_template', 'mad_template'], engine='python')\n",
    "# summ_stats_csv = pd.DataFrame(summ_stats_csv[1:])\n",
    "# summary_list = []\n",
    "# for column, row in summ_stats_csv.iterrows():\n",
    "#     if row['read_id'] in full_keys:\n",
    "#         summary_list.append([row['read_id'], row['mean_qscore_template']])\n",
    "# summary_frame = pd.DataFrame(summary_list)\n",
    "# if args.verbose:\n",
    "#     print('\\033[0;34m' + \"Finished with \" + 'Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt' + '\\033[1;37m')\n",
    "    \n",
    "# Create a dictionary containing the statistics for the filtered dataset\n",
    "    # Total no. frDNA reads, Min. read length, Max. read length, Mean read length, Median read length, Quality score\n",
    "\n",
    "stats_dict = {'number of frDNA reads':len(length_filt_lens),'minimum read length':min(length_filt_lens),'maximum read length':max(length_filt_lens),'mean read length':\"{:.0f}\".format(np.mean(length_filt_lens)),'std dev':\"{:.0f}\".format(np.std(length_filt_lens)),'median read length':\"{:.0f}\".format(np.median(length_filt_lens))\n",
    "#               ,'min_qscore':\"{:.2f}\".format(min(summary_frame[1].astype(float))), 'max_qscore':\"{:.2f}\".format(max(summary_frame[1].astype(float))), 'mean_qscore':\"{:.2f}\".format(np.mean(summary_frame[1].astype(float))), 'median_qscore':\"{:.2f}\".format(np.median(summary_frame[1].astype(float)))\n",
    "             }\n",
    "stats = pd.DataFrame(stats_dict, index=['%s' % '/'.join(args.full_file.rsplit('/')[-3:-1])])    \n",
    "              \n",
    "bx = sns.distplot(length_filt_lens, color=\"k\", kde=False)\n",
    "bx.set(xlim=(250, 3500))\n",
    "bx.set_title(\"frDNA reads for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "bx.set_xlabel(\"Length of read\", fontsize=13)\n",
    "bx.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure2 = bx.get_figure()\n",
    "figure2.savefig('/'.join([output_folder, 'frDNA_len_filt_full.png']))\n",
    "figure2.clf()\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"frDNA spread image file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_full.png']) + '\\033[1;37m')\n",
    "\n",
    "cx = sns.distplot(length_filt_lens, color=\"k\", kde=False)\n",
    "cx.set(xlim=((mean-1.645*std)-100, (mean+1.645*std)+100))\n",
    "cx.set_title(\"frDNA reads for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "cx.set_xlabel(\"Length of read\", fontsize=13)\n",
    "cx.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure3 = cx.get_figure()\n",
    "figure3.savefig('/'.join([output_folder, 'frDNA_len_filt_limited.png']))\n",
    "figure3.clf()\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Zoomed-in frDNA spread image file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_limited.png']) + '\\033[1;37m')\n",
    "\n",
    "stats.to_csv('/'.join([output_folder, 'frDNA_len_filt_statistics.csv']), index=False)\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Summary statistics file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_statistics.csv']) + '\\033[1;37m')\n",
    "    \n",
    "print('\\033[0;35m'+'END'+'\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Stats/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','standard deviation','median read length','min_qscore','max_qscore','mean_qscore','median_qscore'])\n",
    "for path in path_names:\n",
    "    if path[54:-21] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats = full_stats.append(path_stats_csv)\n",
    "            full_stats = full_stats.rename(index={0: path[36:-21]})\n",
    "full_stats = full_stats.sort_index(ascending=True)\n",
    "full_stats.to_csv('../../analysis/Stats/overall_frDNA_stats.csv')\n",
    "num_read_stats = pd.DataFrame(data=[\"{:.0f}\".format(min(full_stats['number of frDNA reads'])), \"{:.0f}\".format(max(full_stats['number of frDNA reads'])), \"{:.0f}\".format(np.mean(full_stats['number of frDNA reads'])), \"{:.0f}\".format(np.median(full_stats['number of frDNA reads']))], index=['Min', 'Max', 'Mean', 'Median'], columns=['Number of reads'])\n",
    "num_read_stats.to_csv('../../analysis/Stats/number_of_frDNA_reads_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(full_stats['number of frDNA reads']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Length_Filtered/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats2 = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','standard deviation','median read length'])\n",
    "for path in path_names:\n",
    "    if path[64:-30] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats2 = full_stats2.append(path_stats_csv)\n",
    "            full_stats2 = full_stats2.rename(index={0: path[46:-30]})\n",
    "full_stats2 = full_stats2.sort_index(ascending=True)\n",
    "display(full_stats2)\n",
    "full_stats2.to_csv('../../analysis/Length_Filtered/overall_frDNA_stats.csv')\n",
    "num_read_stats = pd.DataFrame(data=[\"{:.0f}\".format(min(full_stats2['number of frDNA reads'])), \"{:.0f}\".format(max(full_stats2['number of frDNA reads'])), \"{:.0f}\".format(np.mean(full_stats2['number of frDNA reads'])), \"{:.0f}\".format(np.median(full_stats2['number of frDNA reads']))], index=['Min', 'Max', 'Mean', 'Median'], columns=['Number of reads'])\n",
    "num_read_stats.to_csv('../../analysis/Length_Filtered/number_of_frDNA_reads_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create matrix for loss-of-reads when performing length filtering as percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stats_1 = []\n",
    "full_stats_2 = []\n",
    "result = []\n",
    "\n",
    "for i in full_stats['number of frDNA reads']:\n",
    "    full_stats_1.append(i)\n",
    "for i in full_stats2['number of frDNA reads']:\n",
    "    full_stats_2.append(i)\n",
    "for i in range (0,len(full_stats_1)):\n",
    "    result.append(float(\"{:.2f}\".format((full_stats_1[i]-full_stats_2[i])/full_stats_1[i])))\n",
    "print(result)\n",
    "print('The maximum loss of reads is %s%% in %s' % (100*max(result), path_names[result.index(max(result))][46:-30]))\n",
    "print('The minimum loss of reads is %s%% in %s' % (100*min(result), path_names[result.index(min(result))][46:-30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAH18688_barcode10 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171212_FAH18688/barcode10/merged.fastq\", 'fastq'))\n",
    "lengths = []\n",
    "b10_keys = []\n",
    "for key in FAH18688_barcode10:\n",
    "    lengths.append(len(FAH18688_barcode10[key].seq))\n",
    "    b10_keys.append(key)\n",
    "\n",
    "non_dict_stats = {'number of reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "stats = pd.DataFrame(non_dict_stats, index=['20171212_FAH18688/barcode10'])\n",
    "              \n",
    "ax = sns.distplot(lengths, color=\"k\", kde=False)\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"non-frDNA reads for 20171212_FAH18688/barcode10\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "plt.show()\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAH18654_barcode10 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171207_FAH18654/barcode10/merged.fastq\", 'fastq'))\n",
    "lengths = []\n",
    "b10_keys = []\n",
    "for key in FAH18654_barcode10:\n",
    "    lengths.append(len(FAH18654_barcode10[key].seq))\n",
    "    b10_keys.append(key)\n",
    "\n",
    "non_dict_stats = {'number of reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "stats = pd.DataFrame(non_dict_stats, index=['20171207_FAH18654/barcode10'])\n",
    "              \n",
    "ax = sns.distplot(lengths, color=\"k\", kde=False)\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"non-frDNA reads for 20171207_FAH18654/barcode10\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "plt.show()\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode02 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.fasta\", \"fasta\"))\n",
    "barcode06 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode06/length_restricted_reads.fasta\", \"fasta\"))\n",
    "\n",
    "total_lens = []\n",
    "for key in barcode02:\n",
    "    total_lens.append(len(barcode02[key].seq))\n",
    "for key in barcode06:\n",
    "    total_lens.append(len(barcode06[key].seq))\n",
    "print(max(total_lens))\n",
    "print(min(total_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode02_numbers = {}\n",
    "for key in barcode02:\n",
    "    seq = []\n",
    "    for element in barcode02[key].seq[30:-30]:\n",
    "        if element == \"A\":\n",
    "            seq.append(0)\n",
    "        elif element == \"C\":\n",
    "            seq.append(1)\n",
    "        elif element == \"G\":\n",
    "            seq.append(2)\n",
    "        elif element == \"T\":\n",
    "            seq.append(3)\n",
    "    if len(seq) < max(total_lens):\n",
    "        seq.extend([0]*(max(total_lens)-len(seq)))\n",
    "    barcode02_numbers[key] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2 = np.array(random.choices(list(barcode02_numbers.values()),k=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode06_numbers = {}\n",
    "for key in barcode06:\n",
    "    seq = []\n",
    "    for element in barcode06[key].seq[30:-30]:\n",
    "        if element == \"A\":\n",
    "            seq.append(0)\n",
    "        elif element == \"C\":\n",
    "            seq.append(1)\n",
    "        elif element == \"G\":\n",
    "            seq.append(2)\n",
    "        elif element == \"T\":\n",
    "            seq.append(3)\n",
    "    if len(seq) < max(total_lens):\n",
    "        seq.extend([0]*(max(total_lens)-len(seq)))\n",
    "    barcode06_numbers[key] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq6 = np.array(random.choices(list(barcode06_numbers.values()),k=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_comb = np.concatenate((seq2, seq6), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids2 = np.array([2]*(len(seq2)))\n",
    "print(len([2]*(len(seq2))))\n",
    "ids6 = np.array([6]*(len(seq6)))\n",
    "print(len([2]*(len(seq6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_comb = np.concatenate((ids2, ids6), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seq_comb))\n",
    "print(len(ids_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids.csv', ids_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_comb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs.csv', seq_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_test = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs.csv.npz', allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids.csv.npz', allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Concatenated/*/*/*.fastq\"\n",
    "path_names = glob.glob(path)\n",
    "total_count = 0\n",
    "can_count = 0\n",
    "mis_count = 0\n",
    "unc_count = 0\n",
    "for path in path_names:\n",
    "    temp_dict = SeqIO.to_dict(SeqIO.parse(path, \"fastq\"))\n",
    "    if path[61:-13] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            total_count += len(temp_dict)\n",
    "            can_count += len(temp_dict)\n",
    "        else:\n",
    "            total_count += len(temp_dict)\n",
    "            mis_count += len(temp_dict)\n",
    "            can_count += len(temp_dict)\n",
    "    else:\n",
    "        total_count += len(temp_dict)\n",
    "        unc_count += len(temp_dict)\n",
    "print('Total number of reads assigned to a barcode by Deepbinner is %s' % can_count)\n",
    "print('Number of reads assigned to non-existent barcodes by Deepbinner is %s' % mis_count)\n",
    "print('Estimated number of misassigned reads in total is %s' % (23*mis_count))\n",
    "print('Percentage of misassigned reads based on estimated number is %s' % (2300*(mis_count)/can_count))\n",
    "print('Number of unclassified reads is %s' % unc_count)\n",
    "print('Percentage of unclassified reads is %s' % (100*unc_count/total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Length_Filtered/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats2 = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','std dev','median read length'])\n",
    "for path in path_names:\n",
    "    if path[64:-30] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats2 = full_stats2.append(path_stats_csv)\n",
    "            full_stats2 = full_stats2.rename(index={0: path[46:-30]})\n",
    "full_stats2 = full_stats2.sort_values('mean read length', ascending=False)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(10,17))\n",
    "ax = sns.barplot(x=full_stats2['mean read length'], y=full_stats2.index, palette=\"Blues_d\", xerr=full_stats2['std dev'],ci=True)\n",
    "ax.set_title(\"Mean Read Length by Sample\")\n",
    "figure1 = ax.get_figure()\n",
    "figure1.savefig(\"/10tb/tmp/TE/honours/analysis/Length_Filtered/mean_reads.png\",bbox_inches = \"tight\")\n",
    "figure1.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Length_Filtered/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats2 = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','std dev','median read length'])\n",
    "for path in path_names:\n",
    "    if path[64:-30] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats2 = full_stats2.append(path_stats_csv)\n",
    "            full_stats2 = full_stats2.rename(index={0: path[46:-30]})\n",
    "full_stats2 = full_stats2.sort_values('number of frDNA reads', ascending=False)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(10,17))\n",
    "ax = sns.barplot(x=full_stats2['number of frDNA reads'], y=full_stats2.index, palette=\"Blues_d\")\n",
    "ax.set_title(\"Number of Reads per Sample after Filtering\")\n",
    "figure1 = ax.get_figure()\n",
    "figure1.savefig(\"/10tb/tmp/TE/honours/analysis/Length_Filtered/num_reads.png\",bbox_inches = \"tight\")\n",
    "figure1.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify primers and orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mothur()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.pcr.seqs(fasta=\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.fasta\",oligos=\"../../analysis/ITS_primers.oligos\",pdiffs=0,rdiffs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.pcr.fasta\",\"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "\n",
    "for key in pcr_dict:\n",
    "    ids.append(key)\n",
    "with open('../../analysis/Length_Filtered/20171103_FAH15473/barcode02/ids.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.fasta\",\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "keys_list = random.sample(ids,k=200)\n",
    "print(len(keys_list))\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "print(len(tmp_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../get_cons_ids.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to extract the read ids of reads that\n",
    "contain both the forward and reverse primer as exact matches\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "\n",
    "m = Mothur()\n",
    "m.pcr.seqs(fasta=args.input_file,oligos=args.input_file[:9]+\"ITS_primers.oligos\",pdiffs=0,rdiffs=0)\n",
    "pcr_dict = SeqIO.to_dict(SeqIO.parse(args.input_file[:-5]+\"pcr.fasta\",\"fasta\"))\n",
    "ids = []\n",
    "for key in pcr_dict:\n",
    "    ids.append(key)\n",
    "with open(args.input_file[:-29]+'ids.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)\n",
    "    \n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:-29]+'ids.txt') + '\\033[1;37m')\n",
    "    print('\\033[0;32m' + (\"The number of reads is %s\" % len(ids)) + '\\033[1;37m')\n",
    "    \n",
    "    \n",
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "if len(ids) > 100:\n",
    "    keys_list = random.sample(ids,k=100)\n",
    "else:\n",
    "    print('\\033[1;37m' + \"LOW READS\")\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "SeqIO.write(tmp_dict.values(),(args.input_file[:9]+'Consensus'+args.input_file[24:-29]+'for_consensus.fasta'),'fasta')\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:9]+'Consensus'+args.input_file[24:-29]+'for_consensus.fasta') + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Align import AlignInfo\n",
    "alignment = Bio.AlignIO.read(\"../../analysis/Consensus/20171103_FAH15473/barcode02/consensus_100.fasta\",\"fasta\")\n",
    "summary_align = AlignInfo.SummaryInfo(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus = summary_align.dumb_consensus(threshold=0.7, ambiguous='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(consensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = SeqIO.to_dict(SeqIO.parse(\"../../database/sh_refs_qiime_ver8_dynamic_02.02.2019.fasta\", \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for key in database:\n",
    "    lens.append(len(database[key].seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(lens))\n",
    "print(min(lens))\n",
    "print(np.mean(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of reads in this file is\", len(lens))\n",
    "ax = sns.distplot(lens, color=\"k\", kde=False)\n",
    "\n",
    "\n",
    "# ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Reads spread 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i > 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i > 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i > 3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i > 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i < 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../get_align_seqs.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program extracts a specified number of reads from a fasta\n",
    "file and saves them to a new fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "parser.add_argument(\"num_reads\", help=\"The number of reads to extract\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "    print('\\033[0;31m' + \"The number of extracted reads is \" + '\\033[0;32m' + args.num_reads + '\\033[1;37m')\n",
    "    \n",
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "\n",
    "ids=[]\n",
    "for key in tmp_dict:\n",
    "    ids.append(key)\n",
    "    \n",
    "keys_list = random.sample(ids,k=int(args.num_reads))\n",
    "\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "SeqIO.write(tmp_dict.values(),(args.input_file[:9]+'Alignment'+args.input_file[24:-29]+ args.num_reads + '_reads.fasta'),'fasta')\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:9]+'Alignment'+args.input_file[24:-29]+ args.num_reads + '_reads.fasta') + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emboss_test = \"\"\"nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnAGGTTAnnnnnAAnCAGnnACGAnCT\n",
    "ACnnAAACnnnnGAAnnnTCnGAnnnnCnnAGCAnCnnnnCTGCAAGTCTGGTGCCAGCA\n",
    "GCCnnnnGnCgGnTAATnTCCAGCTCnnnCAATnAGCGnnnnnTAnTATTnnAAAGnnnn\n",
    "nnTTGnnnTTGCnnAGnTTAAAnnAAGCTCnnGTAnnnGTnnTGAAnCCTTGnnGGCnnC\n",
    "TGGCTnnnnGGCCGGnnnTnnnnnCCGCnnCTCACCGnnnnCGTGnnnnnnnTACTnnGG\n",
    "TnCCGnnnnnGCCGGnGCCTTnnnnTnnnnnnnTTCTGGGGAGCCGCnnnnATnnGCCnC\n",
    "TTCACnTnGGGCGnnnTGTnnnnGGGGAACnCAGGACTTTTnnnnnnnnnnAAAAATTAn\n",
    "nnnGAGnTGTTnCnnAAAGCAGGnnCnnnnCTTTGCTCGnnnAAnnTACnATTAGCATGn\n",
    "nGAnATAATnnAnnnGAATAGGnnACGnTGnTnnnGGnnnTTCTATTTTnnGTTGGTTTn\n",
    "nnnnCTnnAGGAnCCnGCCnnGTAAnnnTGnATTAATnAGnnnnGGATAGTnnnCGnnnn\n",
    "GGGGCATnCCnnGTnATTCnnnnAATTGTnCAGAGGnnTnnnGAAAnnTTCTnnTGGAnT\n",
    "TTATTnnGAnnnnAGAnCnGAACnnTACTGCnnnnGAAAGnCATnTTGCCnnAAGGnAnT\n",
    "GnTTTTCAnTnnTAATnCnAGTGAnnnACnnnnGAAAGTTnAGnnGGnnGATCGnnnAAG\n",
    "ACGATCAnnGnATACCnGTCnnnnGTnAGTCTnnnTAAnnnCnCATAAACnTnnnAnnnT\n",
    "GCCGnnnnACnnTAGnGGATCnnGGTnnnGGATGnnTTATCnTTTTnTGACnnTCCATnn\n",
    "nCGnGCACCTnTnACGAnnnGAAATCnAAAGnnnnnnTTTTTGGnnnGTTCTGnGGGGAn\n",
    "nnGTATGGTnCGCnnAAGGCTnGAAnnACTTnAAAGnnAAATTGACnnGGAAnGnnGGCA\n",
    "nCCACCnnAGGCnnGTGnnnnGnnnAGCnCTGCnnnnGGCTTAATTnnnnnTGAnCTnnC\n",
    "AAnnnCACnnnnnGGGAAACTCAnnCCAGGTnCCAGnnACnnAnCAAGTAnGGATTnGAn\n",
    "nnnnnCAGATTGAnGnnAGCTCnnTTTnCTnnTGnnnnnnATTTTnnGTGGnGTGGnTnn\n",
    "nGGnnTGCnnnnnnnnnnnATGnGnnnCCGnTTnnnCTTAGnTTGGTGGnnAGTGAnTTT\n",
    "GTnnCTGnnnnCTTnnAAnTTGCGAnTAAnnCnnnGAAnnCGAGACnnnnnnCTnnnTAn\n",
    "nACnCTGnnnnCTAAnATnnAGCCnnAGGCCCnnGCnnnTTTGGCnnGGGnTnCGCCnnG\n",
    "GCnTTCTnnnTAGAGGnnnnnACTAnnTCnGGCnnTCnnnAAGCCnGATnnGGAAnnnGT\n",
    "TTnnnnnAGGCAAnnTnnnnAACAGGnTnnCTGnnTGATGCnnnCCTnnnnnnTAGAnTG\n",
    "nnnTTCTGnnnGnGCnCGCACGCGCnnnnTACnACTGnnnnAnnnCnnGGAGnCCAACnG\n",
    "nAGnnTTCATnnnnCACCTnTGnnnnnnnGnCCnnGAAAGGTCnnTGGGnTnnnAATCnT\n",
    "TGnnnTTnAnAACTCnnCnnnnnnGTCGnnnnTGCTnnnGGGGATnAnnnnGAGCAnTTG\n",
    "CnnAATTAnnTTGCTCnnnTTCAACnnnnGAGGnAATGnnCCTnnAGnTAAnnnnGCGnC\n",
    "ATGnnTnCATnnCAGCATGCGnnnnTTGAnTTAnnCGTnnCCCTGCCnCTTTnnGTAnCn\n",
    "nnnnACAnCnCGCCCGnnnnTCGnnCnTACTAnnCCGATTGnnnAAnnnnnTGnGnnCTC\n",
    "nnnnAGTGAnnnGGCCnnTTCnnnGGAnnCTGGCTCAnnGGGAGGnnnTnnCGnGCAACn\n",
    "nnnGAnnCCACnCCAGAGnnnnnnnnnnnnCCGGAAnnnnAGTTGnGTnCAAACnTnnCG\n",
    "nnGTCAnnTTTAGnaGnnnGAAGnnnTAAnnAAGnnnnnnnnnTCGTnnAAnnCAAGGTn\n",
    "CTCCGnTnnAnnnGnnGTGAAnCCTGCGGAnGGGATnCnnnATnnnTACnnCnnGAGCGn\n",
    "nnAGGGCnnCTCCGGnGTCnnCGACnnCTCCnAACCCTTTGnnnTGAnAnnnCACATCCn\n",
    "nnnCnnnnGTTGCTTCnnnGGGGnnnnnGCGAnCCnnCTGCCnGGGCGnnCCCCGGnnAG\n",
    "GnnnnnnCACCnnAAAAAACACTGnCATCTnCTGnnCGTnnCGGAnGTTTAnnCGnAGnn\n",
    "nnnnTnAAATCnnGAAACAnnnAAAnnCTTTCAAnnnCAAnnnCnnGGAnnTnnnCTCnT\n",
    "TGGnTTCnTGGCATnnCGAnTGnnAAGAAnnnCGnCnnnnnAGCnnnGAAAnnnnTGCnn\n",
    "GATnnnAAGnTnAATGTGnAnnATTGCAnnnnnGAATTnCnnnAGTnnGAATCnATnnCG\n",
    "nnnAATnnCTTTnGAACnnGCACAnTTGCGCnnCCCCTGnnnnnnGTATTnnCCGGGGnn\n",
    "nnGCAnnTGCCnCGnnnTTCnnGnAGCGnTCAnnnTTACACnnCACTCnCAGnCnnnCTC\n",
    "GCnnnTGGGnnTAnTTGGGCGnnnnnTCTTTTnTCGnnnCGGGGAnTCACTnCCCnCGCG\n",
    "CnnGCCTCAAnnnnnnTCTCCnnnnGGnCTnnnnnGnnAGCGGnnnnnnnTCTCnnGTCT\n",
    "nnnCCCAGCnGTnnTGnTGGCATnCAnnCGTnCTCGnCCGCnnnGGnnnnnnnnGTTnnC\n",
    "nACGAGCCCTnnCACGGnnCCGTTnAnAATnCAnCACCTCAGnnnnGTTnnnGAnCCTnn\n",
    "nCGnGATCGGnnnGTAGGGnATACCnnCGCTnnGAACTTAAnGCnnAnTATnCAATnnnA\n",
    "AGCnnnnGGAnnnGGAAAAnnGnnAAACCAACnnAGGGATTGCCCnnnnTAGnTAAnnCG\n",
    "GCnnnnGAGTnnnnnnnnnGAAGCnnGnnnnGCnnAACnnAGCnnTCnAAATTTnnGAAA\n",
    "TnnnCTGGCCCCnnCGGCnnnCGAnnGTTGTAnnAnnnTTTGnTnnAGAGGAnTGnnnnC\n",
    "TTCTGGGnnTnnnAGCnnGACnnCGnGTnnnnnnCnnnnTAAGTTnnnCCTTnGGAACnn\n",
    "nnAGGAnCnnnGTnCATAGAGnnnnGnnGTGAGAATCCnnCnnnnnGTAnnTGnCGACCn\n",
    "nnnGGCCnnCGCGCCnCTCCAnnnCGnTAGCTCnnCnTTnCnGACnGnAnGTCnnGAGnT\n",
    "TGTTTnGGGnnnAATGCnAGCTCTnnAAATGGGAGnGTAnnAATTTnCTTCTnnnnAnnA\n",
    "AGCTAAATnnACCGnnnGCCnnnnnnnnnAGAGACCnnnGAnTAnnnGCGCnAnCAAGTn\n",
    "nnAnGAGTGATnnnCnGAAAGAnnTGnAnnnnnnAAAGCACnnTTTGGnnnnAAGAGAGT\n",
    "TnnAAAAAGCACGTnnnGAnnnnAATTGnTnnnTGAAAGGGAAnGCGCnTTACnnAAnnC\n",
    "CnAGACnnTTTGGGnnnnnGCGGnnTGTTnnnnnnnCCGCCnnGGnnnTCTTnnCTGAnn\n",
    "nCCnnGGTnCTACTnnnnCTCCGTCCnnnnGAnnGGnnnCCAAnnnnCnnnnnATnnCAT\n",
    "CTnnnGGGAnCCGCCnGGACAnnnAGACCnnTCnAGnGAATnnnGnTAGCTCCCCCnnnn\n",
    "nnGGGAGTnnnnnnnGnTnnTAnnnTAACnnnCTnGTGnnnGTnGATnnGCGGCGnnnnn\n",
    "nCGTCCCGGnnnnnnnnnnTCCGnnCGCTTnnnCGGCAAGnnGATGnTTGGCnnnnGTAA\n",
    "TnnGGTTnnGTCnnnnnAGCGnnGnCCCGTnCTTGAAnnACACGGACCnnnAAGGAGTnn\n",
    "CTAnnnnnACATnnCTATGCGnnAGTGTTCnGGGnnnnnTGnnnTCAnAACCCCTAnCGC\n",
    "nnnGGAATGAnnnnAAGTnnGAACnnnnnnGnnGAGGTnGGGnnnAGGGGCnnAAnnnCC\n",
    "nnCTGnnCAnCCATnnCGAnnnnnnCCGnATCCnnnTGnnATGTnnCnCTCnGnnnnnnn\n",
    "nnnnnnGATGnnnnnnGATTTGnAnnnGTAAGAnnnnGCAnnnTAGCTnGnnnTTnnGGG\n",
    "ACCCGnnnAAAGATGnnGTGnnAACnnTnnnnATGnCCTGAAnnTnAGnnnGGTnnGAnA\n",
    "GCCnnAGAGGAAACnnTCTGGnnTnGGAGGnnCTCGnCAGCnGnnnTnTCTnnnnGACGn\n",
    "nnnTGCnnAAATCnGnnATCnGTCnnnAnnAATTTGGnnnGTAnnTAGGnGnGCGAnnAn\n",
    "AGACTnnAATnCnnGAACnnnCATnCTnnAGnnnTnAGCnnnnnnTGGnnnnnnTTCCnT\n",
    "GCnnCnnnGAAGnTTTnnnCCCTnnCAGGAnTAnnnnnGCnAGnnnnnTAACnnGTTTTn\n",
    "nCAGnTTTTnnnnATGAGGnnTAnnnnnnAAGCnnnnnGAAnTGATTnnAGAGnnnGCnn\n",
    "nnCTTGGGGnTTnnnGAAACAAnCCTnTAnnnACCTATnnTnnCTCAAAnCTTTnnAAAT\n",
    "ATGnTnnAAGnAAGTCCnnTTGnTTnnACTTnAGTTnnnGAACGnnTGGnnAnnnCAnTT\n",
    "TnnnGAATGTnnAnnTCnnnnnGnnnTTACTnAGTnGnnnnGGCCAnTTTTnTGGTAAGC\n",
    "AGAACTGGCGAGnnGnnnTGCnTGnnTCGnATTCCnnGnTTTnnnnGTAnnnGTnnCGTn\n",
    "CnnTGnTTTAACnnnCTTAnGCAnAnnTACnnGnTAACnnnnnnn\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emboss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emboss_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run loop over EMBOSS cons -identity option from 0 to 100 to get a range of different consensus sequences [x]\n",
    "##### Compare each consensus with the GENEIOUS consensus and with ~20 randomly selected reads (same reads for each consensus)\n",
    "###### Extract the score and determine what identity gives the best mean match score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test1.fasta\",\"fasta\"))\n",
    "emboss_test = str(test_dict['test'].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "scores_matrix = []\n",
    "test100 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Alignment/20171103_FAH15473/barcode02/100_reads.fasta\",\"fasta\"))\n",
    "for key in test100:\n",
    "    tmp = []\n",
    "    alignments = pairwise2.align.globalxx(emboss_new, test100[key].seq, score_only=True)\n",
    "    tmp.append(alignments)\n",
    "    scores_matrix.append(int(alignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../consensus_test.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program extracts a specified number of reads from a fasta\n",
    "file and saves them to a new fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "print(emboss_new)\n",
    "\n",
    "scores_matrix = []\n",
    "test100 = SeqIO.to_dict(SeqIO.parse(\"analysis/Alignment/20171103_FAH15473/barcode02/100_reads.fasta\",\"fasta\"))\n",
    "for key in test100:\n",
    "    alignments = pairwise2.align.globalxx(emboss_new, test100[key].seq, score_only=True)\n",
    "    scores_matrix.append(int(alignments))\n",
    "\n",
    "print(scores_matrix)\n",
    "mean = np.mean(scores_matrix)\n",
    "median=np.median(scores_matrix)\n",
    "f=open((args.input_file[:47]+\"scores.txt\"),\"a+\")\n",
    "f.write(\">%s\\t%s\\t%s\\t\\n\" % (keys[0], mean, median))\n",
    "\n",
    "tmp = pd.DataFrame(scores_matrix)\n",
    "tmp.to_csv(args.input_file[:47]+\"%s.csv\" % keys[0],index=False,header=False)\n",
    "\n",
    "\n",
    "with open((args.input_file[:47]+\"%s_new.fasta\" % keys[0]),\"w+\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] +emboss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.input_file[:-29]+'ids.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"../../analysis/Consensus/20171103_FAH15473/barcode02/scores.txt\",\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=contents.replace(\"\\t\",\",\").replace(\"\\n\",\"\").replace(\",>\",\">\").split(\">\")[1:]\n",
    "print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "indices=[]\n",
    "for item in tmp:\n",
    "    indices.append(int(item.split(\",\")[0][4:]))\n",
    "    data.append(((float(item.split(\",\")[1])),float(item.split(\",\")[2])))\n",
    "frame = pd.DataFrame(index=indices,data=data,columns=[\"Mean Score\",\"Median Score\"])\n",
    "print(frame.loc[frame['Mean Score'].idxmax()].name)\n",
    "print(len(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frame.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(frame)\n",
    "    \n",
    "#     # Index 0 is the Geneious-generated Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "geneious = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test0.csv\",header=None)\n",
    "test20 = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test20.csv\",header=None)\n",
    "test1 = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test1.csv\",header=None)\n",
    "test52 = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test52.csv\",header=None)\n",
    "test75 = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test75.csv\",header=None)\n",
    "\n",
    "print(ranksums(geneious,test1))\n",
    "print(ranksums(geneious,test20))\n",
    "print(ranksums(geneious,test52))\n",
    "print(ranksums(geneious,test75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = pd.DataFrame(columns=['geneious','test1','test20','test52','test75'])\n",
    "plot['geneious'] = geneious[0]\n",
    "plot['test1'] = test1[0]\n",
    "plot['test20'] = test20[0]\n",
    "plot['test52'] = test52[0]\n",
    "plot['test75'] = test75[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(data=plot)\n",
    "ax.set_title(\"Different consensus sequence generation methos for 20171103_FAH15473/barcode02\", fontsize=12)\n",
    "ax.set_xlabel(\"Consensus Generation Method\", fontsize=10)\n",
    "ax.set_ylabel(\"Score\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test100 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Alignment/20171103_FAH15473/barcode02/100_reads.fasta\",\"fasta\"))\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test0_new.fasta\",\"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_matrix = []\n",
    "tmp = []\n",
    "for key in test100:\n",
    "    alignments = pairwise2.align.globalxx(emboss_new, test100[key].seq)\n",
    "    scores_matrix.append(alignments)\n",
    "    tmp.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import paf\n",
    "2. Extract columns 10/11 for each row\n",
    "3. Save the alignment percentage identity to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test0.paf\",\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()  \n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp_dict = {}\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[line.split(\",\")[0]] = [int(line.split(\",\")[9]),int(line.split(\",\")[10])]\n",
    "geneious = pd.DataFrame.from_dict(tmp_dict,orient='index',columns=['matching bases','total bases'])\n",
    "geneious['alignment identity'] = 100*(geneious['matching bases']/geneious['total bases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test20.paf\",\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()  \n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp_dict = {}\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[line.split(\",\")[0]] = [int(line.split(\",\")[9]),int(line.split(\",\")[10])]\n",
    "test20 = pd.DataFrame.from_dict(tmp_dict,orient='index',columns=['matching bases','total bases'])\n",
    "test20['alignment identity'] = 100*(test20['matching bases']/test20['total bases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = pd.DataFrame(columns=['geneious','test20'])\n",
    "plot['geneious'] = geneious['alignment identity']\n",
    "plot['test20'] = test20['alignment identity']\n",
    "\n",
    "ax = sns.violinplot(data=plot)\n",
    "ax.set_title(\"Different consensus sequence generation methods for 20171103_FAH15473/barcode02\", fontsize=12)\n",
    "ax.set_xlabel(\"Consensus Generation Method\", fontsize=10)\n",
    "ax.set_ylabel(\"Alignment Percentage Identity\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../cleanup.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program extracts a specified number of reads from a fasta\n",
    "file and saves them to a new fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "with open(args.input_file[:47]+\"clean_consensus.fasta\",\"w+\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] + emboss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../1000_minimap_result.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program extracts a specified number of reads from a fasta\n",
    "file and saves them to a new fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "f=open(args.input_file,\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()\n",
    "\n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp_dict = {}\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[line.split(\",\")[0]] = str(line.split(\",\")[5])\n",
    "\n",
    "count_dict = {}\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for item in tmp_dict:\n",
    "    if tmp_dict[item] not in count_dict:\n",
    "        count_dict[tmp_dict[item]] = 1\n",
    "    else:\n",
    "        count_dict[tmp_dict[item]] = count_dict[tmp_dict[item]] + 1\n",
    "        \n",
    "tmp = pd.DataFrame.from_dict(count_dict,orient='index',columns=[\"Count\"])\n",
    "tmp[\"Percentage Match\"] = tmp.apply(lambda row: 100*row.Count/1000,axis=1)\n",
    "tmp.index.names = ['analysis/Consensus/'+args.input_file[19:-18]]\n",
    "tmp = tmp.sort_values(by=\"Count\",ascending=False)\n",
    "\n",
    "if args.verbose:\n",
    "    print(tmp)\n",
    "\n",
    "tmp.to_csv(args.input_file[:-18]+'match_distribution.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Move all base files from Length_Restricted to the appropriate location\n",
    "    2. For each file, alter the record.name for all elements to ensure each record is associated with the barcode/species/strain (as below) and rewrite the file\n",
    "    3. Create the equal_number files and combined all-read files in the tree as was done above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../record_name.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program alters the record.name of entries in a fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "    \n",
    "with open(args.input_file) as original, open(args.input_file[:-15]+'labelled_read_pool.fasta', 'w') as corrected:\n",
    "    records = SeqIO.parse(original, 'fasta')\n",
    "    for record in records:\n",
    "        record.description = (args.input_file.split('/')[-3]+'_'+args.input_file.split('/')[-2])\n",
    "        SeqIO.write(record, corrected, 'fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basidiomycetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23660\n",
      "23660\n",
      "23660\n",
      "23660\n",
      "215631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215631"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gattii = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/gattii/labelled_read_pool.fasta\", \"fasta\"))\n",
    "neoformans = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/neoformans/labelled_read_pool.fasta\", \"fasta\"))\n",
    "zero = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/zero/labelled_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(gattii), len(neoformans), len(zero)]\n",
    "print(min(lens))\n",
    "\n",
    "new_gattii_keys = random.sample(list(gattii), min(lens))\n",
    "new_neoformans_keys = random.sample(list(neoformans), min(lens))\n",
    "new_zero_keys = random.sample(list(zero), min(lens))\n",
    "\n",
    "new_gattii = {key: gattii[key] for key in new_gattii_keys}\n",
    "new_neoformans = {key: neoformans[key] for key in new_neoformans_keys}\n",
    "new_zero = {key: zero[key] for key in new_zero_keys}\n",
    "\n",
    "print(len(new_gattii))\n",
    "print(len(new_neoformans))\n",
    "print(len(new_zero))\n",
    "\n",
    "full = dict(gattii)\n",
    "full.update(neoformans)\n",
    "full.update(zero)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_gattii.values(), \"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/gattii/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_neoformans.values(), \"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/neoformans/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_zero.values(), \"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/zero/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/full_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/full_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46171\n",
      "46171\n",
      "46171\n",
      "46171\n",
      "46171\n",
      "46171\n",
      "610030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "610030"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Basidiomycota/Agaricomycetes/Agaricales/full_read_pool.fasta\", \"fasta\"))\n",
    "M = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Basidiomycota/Exobasidiomycetes/Microstromatales/full_read_pool.fasta\", \"fasta\"))\n",
    "S = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Basidiomycota/Microbotryomycetes/Sporidiobolales/full_read_pool.fasta\", \"fasta\"))\n",
    "P = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Basidiomycota/Pucciniomycetes/Pucciniales/full_read_pool.fasta\", \"fasta\"))\n",
    "T = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Basidiomycota/Tremellomycetes/Tremellales/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(A), len(M), len(S), len(P), len(T)]\n",
    "print(min(lens))\n",
    "\n",
    "new_A_keys = random.sample(list(A), min(lens))\n",
    "new_M_keys = random.sample(list(M), min(lens))\n",
    "new_S_keys = random.sample(list(S), min(lens))\n",
    "new_P_keys = random.sample(list(P), min(lens))\n",
    "new_T_keys = random.sample(list(T), min(lens))\n",
    "\n",
    "new_A = {key: A[key] for key in new_A_keys}\n",
    "new_M = {key: M[key] for key in new_M_keys}\n",
    "new_S = {key: S[key] for key in new_S_keys}\n",
    "new_P = {key: P[key] for key in new_P_keys}\n",
    "new_T = {key: T[key] for key in new_T_keys}\n",
    "\n",
    "print(len(new_A))\n",
    "print(len(new_M))\n",
    "print(len(new_S))\n",
    "print(len(new_P))\n",
    "print(len(new_T))\n",
    "\n",
    "full = dict(A)\n",
    "full.update(M)\n",
    "full.update(S)\n",
    "full.update(P)\n",
    "full.update(T)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_A.values(), \"../../analysis/Fungi/Basidiomycota/Agaricomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_M.values(), \"../../analysis/Fungi/Basidiomycota/Exobasidiomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_S.values(), \"../../analysis/Fungi/Basidiomycota/Microbotryomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_P.values(), \"../../analysis/Fungi/Basidiomycota/Pucciniomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_T.values(), \"../../analysis/Fungi/Basidiomycota/Tremellomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Basidiomycota/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ascomycota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44257\n",
      "44257\n",
      "44257\n",
      "44257\n",
      "267361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "267361"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Dothideomycetes/Botryosphaeriales/Botryosphaeriaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "C = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Dothideomycetes/Capnodiales/Mycosphaerellaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "P = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Dothideomycetes/Pleosporales/Pleosporaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(B), len(C), len(P)]\n",
    "print(min(lens))\n",
    "\n",
    "new_B_keys = random.sample(list(B), min(lens))\n",
    "new_C_keys = random.sample(list(C), min(lens))\n",
    "new_P_keys = random.sample(list(P), min(lens))\n",
    "\n",
    "new_B = {key: B[key] for key in new_B_keys}\n",
    "new_C = {key: C[key] for key in new_C_keys}\n",
    "new_P = {key: P[key] for key in new_P_keys}\n",
    "\n",
    "print(len(new_B))\n",
    "print(len(new_C))\n",
    "print(len(new_P))\n",
    "\n",
    "full = dict(B)\n",
    "full.update(C)\n",
    "full.update(P)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_B.values(), \"../../analysis/Fungi/Ascomycota/Dothideomycetes/Botryosphaeriales/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_C.values(), \"../../analysis/Fungi/Ascomycota/Dothideomycetes/Capnodiales/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_P.values(), \"../../analysis/Fungi/Ascomycota/Dothideomycetes/Pleosporales/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Dothideomycetes/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39988\n",
      "39988\n",
      "39988\n",
      "39988\n",
      "150734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150734"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/flavus/labelled_read_pool.fasta\", \"fasta\"))\n",
    "N = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/niger/labelled_read_pool.fasta\", \"fasta\"))\n",
    "S = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/sp./labelled_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(F), len(N), len(S)]\n",
    "print(min(lens))\n",
    "\n",
    "new_F_keys = random.sample(list(F), min(lens))\n",
    "new_N_keys = random.sample(list(N), min(lens))\n",
    "new_S_keys = random.sample(list(S), min(lens))\n",
    "\n",
    "new_F = {key: F[key] for key in new_F_keys}\n",
    "new_N = {key: N[key] for key in new_N_keys}\n",
    "new_S = {key: S[key] for key in new_S_keys}\n",
    "\n",
    "print(len(new_F))\n",
    "print(len(new_N))\n",
    "print(len(new_S))\n",
    "\n",
    "full = dict(F)\n",
    "full.update(N)\n",
    "full.update(S)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_F.values(), \"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/flavus/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_N.values(), \"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/flavus/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_S.values(), \"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/flavus/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/full_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25917\n",
      "25917\n",
      "25917\n",
      "176651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "176651"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Chaetothyriales/Herpotrichiellaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "E = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(C), len(E)]\n",
    "print(min(lens))\n",
    "\n",
    "new_C_keys = random.sample(list(C), min(lens))\n",
    "new_E_keys = random.sample(list(E), min(lens))\n",
    "\n",
    "new_C = {key: C[key] for key in new_C_keys}\n",
    "new_E = {key: E[key] for key in new_E_keys}\n",
    "\n",
    "print(len(new_C))\n",
    "print(len(new_E))\n",
    "\n",
    "full = dict(C)\n",
    "full.update(E)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_C.values(), \"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Chaetothyriales/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_E.values(), \"../../analysis/Fungi/Ascomycota/Eurotiomycetes/Eurotiales/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Eurotiomycetes/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49481\n",
      "49481\n",
      "49481\n",
      "105070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105070"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t29 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Tapesia/yallundae/CCL029/labelled_read_pool.fasta\", \"fasta\"))\n",
    "t31 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Tapesia/yallundae/CCL031/labelled_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(t29), len(t31)]\n",
    "print(min(lens))\n",
    "\n",
    "new_t29_keys = random.sample(list(t29), min(lens))\n",
    "new_t31_keys = random.sample(list(t31), min(lens))\n",
    "\n",
    "new_t29 = {key: t29[key] for key in new_t29_keys}\n",
    "new_t31 = {key: t31[key] for key in new_t31_keys}\n",
    "\n",
    "print(len(new_t29))\n",
    "print(len(new_t31))\n",
    "\n",
    "full = dict(t29)\n",
    "full.update(t31)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_t29.values(), \"../../analysis/Fungi/Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Tapesia/yallundae/CCL029/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_t31.values(), \"../../analysis/Fungi/Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Tapesia/yallundae/CCL031/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Tapesia/yallundae/full_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Tapesia/full_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/full_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Leotiomycetes/Helotiales/full_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Leotiomycetes/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34841\n",
      "34841\n",
      "34841\n",
      "77210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77210"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/mexicana/labelled_read_pool.fasta\", \"fasta\"))\n",
    "s = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/scolyti/labelled_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(m), len(s)]\n",
    "print(min(lens))\n",
    "\n",
    "new_m_keys = random.sample(list(m), min(lens))\n",
    "new_s_keys = random.sample(list(s), min(lens))\n",
    "\n",
    "new_m = {key: m[key] for key in new_m_keys}\n",
    "new_s = {key: s[key] for key in new_s_keys}\n",
    "\n",
    "print(len(new_m))\n",
    "print(len(new_s))\n",
    "\n",
    "full = dict(m)\n",
    "full.update(s)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_m.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/mexicana/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_s.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/scolyti/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18944\n",
      "18944\n",
      "18944\n",
      "96154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96154"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Meyerozyma/full_read_pool.fasta\", \"fasta\"))\n",
    "y = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(m), len(y)]\n",
    "print(min(lens))\n",
    "\n",
    "new_m_keys = random.sample(list(m), min(lens))\n",
    "new_y_keys = random.sample(list(y), min(lens))\n",
    "\n",
    "new_m = {key: m[key] for key in new_m_keys}\n",
    "new_y = {key: y[key] for key in new_y_keys}\n",
    "\n",
    "print(len(new_m))\n",
    "print(len(new_y))\n",
    "\n",
    "full = dict(m)\n",
    "full.update(y)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_m.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Meyerozyma/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_y.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7805\n",
      "7805\n",
      "7805\n",
      "7805\n",
      "68773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68773"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Galactomyces/full_read_pool.fasta\", \"fasta\"))\n",
    "ge = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Geotrichum/full_read_pool.fasta\", \"fasta\"))\n",
    "y = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Yarrowia/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(ga), len(ge), len(y)]\n",
    "print(min(lens))\n",
    "\n",
    "new_ga_keys = random.sample(list(ga), min(lens))\n",
    "new_ge_keys = random.sample(list(ge), min(lens))\n",
    "new_y_keys = random.sample(list(y), min(lens))\n",
    "\n",
    "new_ga = {key: ga[key] for key in new_ga_keys}\n",
    "new_ge = {key: ge[key] for key in new_ge_keys}\n",
    "new_y = {key: y[key] for key in new_y_keys}\n",
    "\n",
    "print(len(new_ga))\n",
    "print(len(new_ge))\n",
    "print(len(new_y))\n",
    "\n",
    "full = dict(ga)\n",
    "full.update(ge)\n",
    "full.update(y)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_ga.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Galactomyces/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_ge.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Geotrichum/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_y.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Yarrowia/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36478\n",
      "36478\n",
      "36478\n",
      "168414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "168414"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Clavispora/full_read_pool.fasta\", \"fasta\"))\n",
    "k = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Kodamaea/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(c), len(k)]\n",
    "print(min(lens))\n",
    "\n",
    "new_c_keys = random.sample(list(c), min(lens))\n",
    "new_k_keys = random.sample(list(k), min(lens))\n",
    "\n",
    "new_c = {key: c[key] for key in new_c_keys}\n",
    "new_k = {key: k[key] for key in new_k_keys}\n",
    "\n",
    "print(len(new_c))\n",
    "print(len(new_k))\n",
    "\n",
    "full = dict(c)\n",
    "full.update(k)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_c.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Clavispora/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_k.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Kodamaea/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26937\n",
      "26937\n",
      "26937\n",
      "60181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60181"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/membranifaciens/labelled_read_pool.fasta\", \"fasta\"))\n",
    "k = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/kudriavzevii/labelled_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(m), len(k)]\n",
    "print(min(lens))\n",
    "\n",
    "new_m_keys = random.sample(list(m), min(lens))\n",
    "new_k_keys = random.sample(list(k), min(lens))\n",
    "\n",
    "new_m = {key: m[key] for key in new_m_keys}\n",
    "new_k = {key: k[key] for key in new_k_keys}\n",
    "\n",
    "print(len(new_m))\n",
    "print(len(new_k))\n",
    "\n",
    "full = dict(m)\n",
    "full.update(k)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_m.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/membranifaciens/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_k.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/kudriavzevii/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/full_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25597\n",
      "25597\n",
      "25597\n",
      "25597\n",
      "25597\n",
      "25597\n",
      "168625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "168625"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/albicans/labelled_read_pool.fasta\", \"fasta\"))\n",
    "m = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/metapsilosis/labelled_read_pool.fasta\", \"fasta\"))\n",
    "o = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/orthopsilosis/labelled_read_pool.fasta\", \"fasta\"))\n",
    "p = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/parapsilosis/labelled_read_pool.fasta\", \"fasta\"))\n",
    "z = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/zeylanoides/labelled_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(a), len(m),len(o),len(p),len(z)]\n",
    "print(min(lens))\n",
    "\n",
    "new_a_keys = random.sample(list(a), min(lens))\n",
    "new_m_keys = random.sample(list(m), min(lens))\n",
    "new_o_keys = random.sample(list(o), min(lens))\n",
    "new_p_keys = random.sample(list(p), min(lens))\n",
    "new_z_keys = random.sample(list(z), min(lens))\n",
    "\n",
    "new_a = {key: a[key] for key in new_a_keys}\n",
    "new_m = {key: m[key] for key in new_m_keys}\n",
    "new_o = {key: o[key] for key in new_o_keys}\n",
    "new_p = {key: p[key] for key in new_p_keys}\n",
    "new_z = {key: z[key] for key in new_z_keys}\n",
    "\n",
    "print(len(new_a))\n",
    "print(len(new_m))\n",
    "print(len(new_o))\n",
    "print(len(new_p))\n",
    "print(len(new_z))\n",
    "\n",
    "full = dict(a)\n",
    "full.update(m)\n",
    "full.update(o)\n",
    "full.update(p)\n",
    "full.update(z)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_a.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/albicans/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_m.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/metapsilosis/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_o.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/orthopsilosis/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_p.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/parapsilosis/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_z.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/zeylanoides/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28382\n",
      "28382\n",
      "28382\n",
      "63502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63502"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/marxianus/labelled_read_pool.fasta\", \"fasta\"))\n",
    "l = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/lactis/labelled_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(m), len(l)]\n",
    "print(min(lens))\n",
    "\n",
    "new_m_keys = random.sample(list(m), min(lens))\n",
    "new_l_keys = random.sample(list(l), min(lens))\n",
    "\n",
    "new_m = {key: m[key] for key in new_m_keys}\n",
    "new_l = {key: l[key] for key in new_l_keys}\n",
    "\n",
    "print(len(new_m))\n",
    "print(len(new_l))\n",
    "\n",
    "full = dict(m)\n",
    "full.update(l)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_m.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/marxianus/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_l.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/lactis/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30260\n",
      "30260\n",
      "30260\n",
      "30260\n",
      "262387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "262387"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/full_read_pool.fasta\", \"fasta\"))\n",
    "k = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/full_read_pool.fasta\", \"fasta\"))\n",
    "s = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Saccharomyces/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(c), len(k), len(s)]\n",
    "print(min(lens))\n",
    "\n",
    "new_c_keys = random.sample(list(c), min(lens))\n",
    "new_k_keys = random.sample(list(k), min(lens))\n",
    "new_s_keys = random.sample(list(s), min(lens))\n",
    "\n",
    "new_c = {key: c[key] for key in new_c_keys}\n",
    "new_k = {key: k[key] for key in new_k_keys}\n",
    "new_s = {key: s[key] for key in new_s_keys}\n",
    "\n",
    "print(len(new_c))\n",
    "print(len(new_k))\n",
    "print(len(new_s))\n",
    "\n",
    "full = dict(c)\n",
    "full.update(k)\n",
    "full.update(s)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_c.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_k.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_s.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Saccharomyces/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34002\n",
      "34002\n",
      "34002\n",
      "71317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71317"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Blastobotrys/full_read_pool.fasta\", \"fasta\"))\n",
    "z = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Zygoascus/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(b), len(z)]\n",
    "print(min(lens))\n",
    "\n",
    "new_b_keys = random.sample(list(b), min(lens))\n",
    "new_z_keys = random.sample(list(z), min(lens))\n",
    "\n",
    "new_b = {key: b[key] for key in new_b_keys}\n",
    "new_z = {key: z[key] for key in new_z_keys}\n",
    "\n",
    "print(len(new_b))\n",
    "print(len(new_z))\n",
    "\n",
    "full = dict(b)\n",
    "full.update(z)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_b.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Blastobotrys/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_z.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Zygoascus/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42589\n",
      "42589\n",
      "42589\n",
      "42589\n",
      "42589\n",
      "42589\n",
      "42589\n",
      "42589\n",
      "769815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "769815"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "di = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "m = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "ph = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Phaffomycetaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "pi = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "s = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "t = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(de), len(di),len(m),len(ph),len(pi),len(s),len(t)]\n",
    "print(min(lens))\n",
    "\n",
    "new_de_keys = random.sample(list(de), min(lens))\n",
    "new_di_keys = random.sample(list(di), min(lens))\n",
    "new_m_keys = random.sample(list(m), min(lens))\n",
    "new_ph_keys = random.sample(list(ph), min(lens))\n",
    "new_pi_keys = random.sample(list(pi), min(lens))\n",
    "new_s_keys = random.sample(list(s), min(lens))\n",
    "new_t_keys = random.sample(list(t), min(lens))\n",
    "\n",
    "new_de = {key: de[key] for key in new_de_keys}\n",
    "new_di = {key: di[key] for key in new_di_keys}\n",
    "new_m = {key: m[key] for key in new_m_keys}\n",
    "new_ph = {key: ph[key] for key in new_ph_keys}\n",
    "new_pi = {key: pi[key] for key in new_pi_keys}\n",
    "new_s = {key: s[key] for key in new_s_keys}\n",
    "new_t = {key: t[key] for key in new_t_keys}\n",
    "\n",
    "print(len(new_de))\n",
    "print(len(new_di))\n",
    "print(len(new_m))\n",
    "print(len(new_ph))\n",
    "print(len(new_pi))\n",
    "print(len(new_s))\n",
    "print(len(new_t))\n",
    "\n",
    "full = dict(de)\n",
    "full.update(di)\n",
    "full.update(m)\n",
    "full.update(ph)\n",
    "full.update(pi)\n",
    "full.update(s)\n",
    "full.update(t)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_de.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_di.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_m.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_ph.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Phaffomycetaceae/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_pi.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_s.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_t.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/Saccharomycetales/full_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27941\n",
      "27941\n",
      "27941\n",
      "27941\n",
      "27941\n",
      "276762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "276762"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Sordariomycetes/Diaporthales/full_read_pool.fasta\", \"fasta\"))\n",
    "h = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Sordariomycetes/Hypocreales/full_read_pool.fasta\", \"fasta\"))\n",
    "m = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Sordariomycetes/Microascales/full_read_pool.fasta\", \"fasta\"))\n",
    "x = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Sordariomycetes/Xylariales/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(d), len(h), len(m),len(x)]\n",
    "print(min(lens))\n",
    "\n",
    "new_d_keys = random.sample(list(d), min(lens))\n",
    "new_h_keys = random.sample(list(h), min(lens))\n",
    "new_m_keys = random.sample(list(m), min(lens))\n",
    "new_x_keys = random.sample(list(x), min(lens))\n",
    "\n",
    "new_d = {key: d[key] for key in new_d_keys}\n",
    "new_h = {key: h[key] for key in new_h_keys}\n",
    "new_m = {key: m[key] for key in new_m_keys}\n",
    "new_x = {key: x[key] for key in new_x_keys}\n",
    "\n",
    "print(len(new_d))\n",
    "print(len(new_h))\n",
    "print(len(new_m))\n",
    "print(len(new_x))\n",
    "\n",
    "full = dict(d)\n",
    "full.update(h)\n",
    "full.update(m)\n",
    "full.update(x)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_d.values(), \"../../analysis/Fungi/Ascomycota/Sordariomycetes/Diaporthales/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_h.values(), \"../../analysis/Fungi/Ascomycota/Sordariomycetes/Hypocreales/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_m.values(), \"../../analysis/Fungi/Ascomycota/Sordariomycetes/Microascales/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_x.values(), \"../../analysis/Fungi/Ascomycota/Sordariomycetes/Xylariales/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/Sordariomycetes/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74232\n",
      "74232\n",
      "74232\n",
      "74232\n",
      "74232\n",
      "74232\n",
      "74232\n",
      "1669891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1669891"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Dothideomycetes/full_read_pool.fasta\", \"fasta\"))\n",
    "e = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Eurotiomycetes/full_read_pool.fasta\", \"fasta\"))\n",
    "l = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Leotiomycetes/full_read_pool.fasta\", \"fasta\"))\n",
    "p = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Pezizomycetes/full_read_pool.fasta\", \"fasta\"))\n",
    "sa = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Saccharomycetes/full_read_pool.fasta\", \"fasta\"))\n",
    "so = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/Sordariomycetes/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(d), len(e),len(l),len(p),len(sa),len(so)]\n",
    "print(min(lens))\n",
    "\n",
    "new_d_keys = random.sample(list(d), min(lens))\n",
    "new_e_keys = random.sample(list(e), min(lens))\n",
    "new_l_keys = random.sample(list(l), min(lens))\n",
    "new_p_keys = random.sample(list(p), min(lens))\n",
    "new_sa_keys = random.sample(list(sa), min(lens))\n",
    "new_so_keys = random.sample(list(so), min(lens))\n",
    "\n",
    "new_d = {key: d[key] for key in new_d_keys}\n",
    "new_e = {key: e[key] for key in new_e_keys}\n",
    "new_l = {key: l[key] for key in new_l_keys}\n",
    "new_p = {key: p[key] for key in new_p_keys}\n",
    "new_sa = {key: sa[key] for key in new_sa_keys}\n",
    "new_so = {key: so[key] for key in new_so_keys}\n",
    "\n",
    "print(len(new_d))\n",
    "print(len(new_e))\n",
    "print(len(new_l))\n",
    "print(len(new_p))\n",
    "print(len(new_sa))\n",
    "print(len(new_so))\n",
    "\n",
    "full = dict(d)\n",
    "full.update(e)\n",
    "full.update(l)\n",
    "full.update(p)\n",
    "full.update(sa)\n",
    "full.update(so)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_d.values(), \"../../analysis/Fungi/Ascomycota/Dothideomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_e.values(), \"../../analysis/Fungi/Ascomycota/Eurotiomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_l.values(), \"../../analysis/Fungi/Ascomycota/Leotiomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_p.values(), \"../../analysis/Fungi/Ascomycota/Pezizomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_sa.values(), \"../../analysis/Fungi/Ascomycota/Saccharomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_so.values(), \"../../analysis/Fungi/Ascomycota/Sordariomycetes/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/Ascomycota/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610030\n",
      "610030\n",
      "610030\n",
      "2279921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2279921"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Ascomycota/full_read_pool.fasta\", \"fasta\"))\n",
    "b = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Fungi/Basidiomycota/full_read_pool.fasta\", \"fasta\"))\n",
    "lens = [len(a), len(b)]\n",
    "print(min(lens))\n",
    "\n",
    "new_a_keys = random.sample(list(a), min(lens))\n",
    "new_b_keys = random.sample(list(b), min(lens))\n",
    "\n",
    "new_a = {key: a[key] for key in new_a_keys}\n",
    "new_b = {key: b[key] for key in new_b_keys}\n",
    "\n",
    "print(len(new_a))\n",
    "print(len(new_b))\n",
    "\n",
    "full = dict(a)\n",
    "full.update(b)\n",
    "print(len(full))\n",
    "\n",
    "SeqIO.write(new_a.values(), \"../../analysis/Fungi/Ascomycota//equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(new_b.values(), \"../../analysis/Fungi/Basidiomycota/equal_read_pool.fasta\", \"fasta\")\n",
    "SeqIO.write(full.values(), \"../../analysis/Fungi/full_read_pool.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
