{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Z. tritici vs C. globuliformis\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import plot_model\n",
    "# from PIL import Image\n",
    "# import random\n",
    "# import math\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# # First import the data by loading the file and extracting the correct section\n",
    "# data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs.csv.npz')\n",
    "# data = data_npz['arr_0']\n",
    "# labels_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids.csv.npz')\n",
    "# labels = labels_npz['arr_0']\n",
    "\n",
    "# # Print the shape of the resulting dataframes to visually verify\n",
    "# print(labels.shape)\n",
    "# print(data.shape)\n",
    "# print(data)\n",
    "\n",
    "# # Separate the data into separate classes based on the labels\n",
    "# data_class_2 = data[(labels == 2)]\n",
    "# data_class_6 = data[(labels == 6)]\n",
    "# # Print an entry to visualise this\n",
    "# print(data_class_2[50])\n",
    "# print(data_class_6[50])\n",
    "\n",
    "# # Print the shape of these new arrays to visually verify\n",
    "# print('data_class_2.shape: ', data_class_2.shape)\n",
    "# print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# # Determine the total number of samples per class, and the total number of samples overall\n",
    "# samples_per_class = data_class_2.shape[0]\n",
    "# samples_count = samples_per_class*2\n",
    "# print('samples_per_class: ', samples_per_class)\n",
    "# print('samples_count: ', samples_count)\n",
    "\n",
    "# # Create a vertically stacked arra containing all sequences, then join labels\n",
    "# all_data = np.vstack((data_class_2, data_class_6))\n",
    "# print('all_data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "# print('all_labels.shape : ', all_labels.shape)\n",
    "\n",
    "# # Create a method for shuffling data\n",
    "# shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "# print(len(shuffle_indices))\n",
    "\n",
    "# # Assign a percentage of data for training and the rest for testing\n",
    "# train_size = math.floor(0.85*all_data.shape[0])\n",
    "# indices_train = shuffle_indices[0:train_size]\n",
    "# indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# # Define the data vs labels for each of the training ad test sets\n",
    "# X_train = all_data[indices_train,:]\n",
    "# Y_train = all_labels[indices_train]\n",
    "# X_test = all_data[indices_test,:]\n",
    "# Y_test = all_labels[indices_test]\n",
    "# print('X_train.shape : ', X_train.shape)\n",
    "# print('X_test.shape : ', X_test.shape)\n",
    "# print('Y_train.shape : ', Y_train.shape)\n",
    "# print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# # Define the input dimension from X_train.shape[1]\n",
    "# in_dim = X_train.shape[1]\n",
    "\n",
    "# # define the keras model\n",
    "# #model = Sequential()\n",
    "# #model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "# #model.add(Dense(64, activation='relu'))\n",
    "# #model.add(Dense(32, activation='relu'))\n",
    "# #model.add(Dense(2, activation='softmax'))\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # compile the keras model\n",
    "# #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Run the model\n",
    "# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Z. tritici vs P. tritici-repentis\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import plot_model\n",
    "# from PIL import Image\n",
    "# import random\n",
    "# import math\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# # First import the data by loading the file and extracting the correct section\n",
    "# data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b3_seqs.csv.npz')\n",
    "# data = data_npz['arr_0']\n",
    "# labels_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b3_ids.csv.npz')\n",
    "# labels = labels_npz['arr_0']\n",
    "\n",
    "# # Print the shape of the resulting dataframes to visually verify\n",
    "# print(data.shape)\n",
    "# print(labels.shape)\n",
    "\n",
    "# # Separate the data into separate classes based on the labels\n",
    "# data_class_2 = data[(labels == 2)]\n",
    "# data_class_3 = data[(labels == 3)]\n",
    "# # Print an entry to visualise this\n",
    "# print(data_class_2[50])\n",
    "# print(data_class_3[50])\n",
    "\n",
    "# # Print the shape of these new arrays to visually verify\n",
    "# print('data_class_2.shape: ', data_class_2.shape)\n",
    "# print('data_class_3.shape: ', data_class_3.shape)\n",
    "\n",
    "# # Determine the total number of samples per class, and the total number of samples overall\n",
    "# samples_per_class = data_class_2.shape[0]\n",
    "# samples_count = samples_per_class*2\n",
    "# print('samples_per_class: ', samples_per_class)\n",
    "# print('samples_count: ', samples_count)\n",
    "\n",
    "# # Create a vertically stacked arra containing all sequences, then join labels\n",
    "# all_data = np.vstack((data_class_2, data_class_3))\n",
    "# print('all_data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "# print('all_labels.shape : ', all_labels.shape)\n",
    "\n",
    "# # Create a method for shuffling data\n",
    "# shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "# print(len(shuffle_indices))\n",
    "\n",
    "# # Assign a percentage of data for training and the rest for testing\n",
    "# train_size = math.floor(0.85*all_data.shape[0])\n",
    "# indices_train = shuffle_indices[0:train_size]\n",
    "# indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# # Define the data vs labels for each of the training ad test sets\n",
    "# X_train = all_data[indices_train,:]\n",
    "# Y_train = all_labels[indices_train]\n",
    "# X_test = all_data[indices_test,:]\n",
    "# Y_test = all_labels[indices_test]\n",
    "# print('X_train.shape : ', X_train.shape)\n",
    "# print('X_test.shape : ', X_test.shape)\n",
    "# print('Y_train.shape : ', Y_train.shape)\n",
    "# print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# # Define the input dimension from X_train.shape[1]\n",
    "# in_dim = X_train.shape[1]\n",
    "\n",
    "# # define the keras model\n",
    "# #model = Sequential()\n",
    "# #model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "# #model.add(Dense(64, activation='relu'))\n",
    "# #model.add(Dense(32, activation='relu'))\n",
    "# #model.add(Dense(2, activation='softmax'))\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # compile the keras model\n",
    "# #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Run the model\n",
    "# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # C. metapsilosis vs C. orthopsilosis\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import plot_model\n",
    "# from PIL import Image\n",
    "# import random\n",
    "# import math\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# # First import the data by loading the file and extracting the correct section\n",
    "# data_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5_seqs.csv.npz')\n",
    "# data = data_npz['arr_0']\n",
    "# labels_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5_ids.csv.npz')\n",
    "# labels = labels_npz['arr_0']\n",
    "\n",
    "# # Print the shape of the resulting dataframes to visually verify\n",
    "# print(labels.shape)\n",
    "# print(data.shape)\n",
    "\n",
    "# # Separate the data into separate classes based on the labels\n",
    "# data_class_4 = data[(labels == 4)]\n",
    "# data_class_5 = data[(labels == 5)]\n",
    "# # Print an entry to visualise this\n",
    "# print(data_class_4[50])\n",
    "# print(data_class_5[50])\n",
    "\n",
    "# # Print the shape of these new arrays to visually verify\n",
    "# print('data_class_4.shape: ', data_class_4.shape)\n",
    "# print('data_class_5.shape: ', data_class_5.shape)\n",
    "\n",
    "# # Determine the total number of samples per class, and the total number of samples overall\n",
    "# samples_per_class = data_class_4.shape[0]\n",
    "# samples_count = samples_per_class*2\n",
    "# print('samples_per_class: ', samples_per_class)\n",
    "# print('samples_count: ', samples_count)\n",
    "\n",
    "# # Create a vertically stacked arra containing all sequences, then join labels\n",
    "# all_data = np.vstack((data_class_4, data_class_5))\n",
    "# print('all_data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "# print('all_labels.shape : ', all_labels.shape)\n",
    "\n",
    "# # Create a method for shuffling data\n",
    "# shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "# print(len(shuffle_indices))\n",
    "\n",
    "# # Assign a percentage of data for training and the rest for testing\n",
    "# train_size = math.floor(0.85*all_data.shape[0])\n",
    "# indices_train = shuffle_indices[0:train_size]\n",
    "# indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# # Define the data vs labels for each of the training ad test sets\n",
    "# X_train = all_data[indices_train,:]\n",
    "# Y_train = all_labels[indices_train]\n",
    "# X_test = all_data[indices_test,:]\n",
    "# Y_test = all_labels[indices_test]\n",
    "# print('X_train.shape : ', X_train.shape)\n",
    "# print('X_test.shape : ', X_test.shape)\n",
    "# print('Y_train.shape : ', Y_train.shape)\n",
    "# print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# # Define the input dimension from X_train.shape[1]\n",
    "# in_dim = X_train.shape[1]\n",
    "\n",
    "# # define the keras model\n",
    "# #model = Sequential()\n",
    "# #model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "# #model.add(Dense(64, activation='relu'))\n",
    "# #model.add(Dense(32, activation='relu'))\n",
    "# #model.add(Dense(2, activation='softmax'))\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # compile the keras model\n",
    "# #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Run the model\n",
    "# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.savefig('../../cryptic_4.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # C. metapsilosis vs C. orthopsilosis\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import plot_model\n",
    "# from PIL import Image\n",
    "# import random\n",
    "# import math\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# # First import the data by loading the file and extracting the correct section\n",
    "# data_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5_0_seqs.csv.npz')\n",
    "# data = data_npz['arr_0']\n",
    "# labels_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5_0_ids.csv.npz')\n",
    "# labels = labels_npz['arr_0']\n",
    "\n",
    "# # Print the shape of the resulting dataframes to visually verify\n",
    "# print(labels.shape)\n",
    "# print(data.shape)\n",
    "\n",
    "# # Separate the data into separate classes based on the labels\n",
    "# data_class_4 = data[(labels == 4)]\n",
    "# data_class_5 = data[(labels == 5)]\n",
    "# # Print an entry to visualise this\n",
    "# print(data_class_4[50])\n",
    "# print(data_class_5[50])\n",
    "\n",
    "# # Print the shape of these new arrays to visually verify\n",
    "# print('data_class_4.shape: ', data_class_4.shape)\n",
    "# print('data_class_5.shape: ', data_class_5.shape)\n",
    "\n",
    "# # Determine the total number of samples per class, and the total number of samples overall\n",
    "# samples_per_class = data_class_4.shape[0]\n",
    "# samples_count = samples_per_class*2\n",
    "# print('samples_per_class: ', samples_per_class)\n",
    "# print('samples_count: ', samples_count)\n",
    "\n",
    "# # Create a vertically stacked arra containing all sequences, then join labels\n",
    "# all_data = np.vstack((data_class_4, data_class_5))\n",
    "# print('all_data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "# print('all_labels.shape : ', all_labels.shape)\n",
    "\n",
    "# # Create a method for shuffling data\n",
    "# shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "# print(len(shuffle_indices))\n",
    "\n",
    "# # Assign a percentage of data for training and the rest for testing\n",
    "# train_size = math.floor(0.85*all_data.shape[0])\n",
    "# indices_train = shuffle_indices[0:train_size]\n",
    "# indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# # Define the data vs labels for each of the training ad test sets\n",
    "# X_train = all_data[indices_train,:]\n",
    "# Y_train = all_labels[indices_train]\n",
    "# X_test = all_data[indices_test,:]\n",
    "# Y_test = all_labels[indices_test]\n",
    "# print('X_train.shape : ', X_train.shape)\n",
    "# print('X_test.shape : ', X_test.shape)\n",
    "# print('Y_train.shape : ', Y_train.shape)\n",
    "# print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# # Define the input dimension from X_train.shape[1]\n",
    "# in_dim = X_train.shape[1]\n",
    "\n",
    "# # define the keras model\n",
    "# #model = Sequential()\n",
    "# #model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "# #model.add(Dense(64, activation='relu'))\n",
    "# #model.add(Dense(32, activation='relu'))\n",
    "# #model.add(Dense(2, activation='softmax'))\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # compile the keras model\n",
    "# #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Run the model\n",
    "# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.savefig('../../cryptic_0.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # C. metapsilosis vs C. orthopsilosis\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import plot_model\n",
    "# from PIL import Image\n",
    "# import random\n",
    "# import math\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# # First import the data by loading the file and extracting the correct section\n",
    "# data_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5_1-4_seqs.csv.npz')\n",
    "# data = data_npz['arr_0']\n",
    "# labels_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5_1-4_ids.csv.npz')\n",
    "# labels = labels_npz['arr_0']\n",
    "\n",
    "# # Print the shape of the resulting dataframes to visually verify\n",
    "# print(labels.shape)\n",
    "# print(data.shape)\n",
    "\n",
    "# # Separate the data into separate classes based on the labels\n",
    "# data_class_4 = data[(labels == 4)]\n",
    "# data_class_5 = data[(labels == 5)]\n",
    "# # Print an entry to visualise this\n",
    "# print(data_class_4[50])\n",
    "# print(data_class_5[50])\n",
    "\n",
    "# # Print the shape of these new arrays to visually verify\n",
    "# print('data_class_4.shape: ', data_class_4.shape)\n",
    "# print('data_class_5.shape: ', data_class_5.shape)\n",
    "\n",
    "# # Determine the total number of samples per class, and the total number of samples overall\n",
    "# samples_per_class = data_class_4.shape[0]\n",
    "# samples_count = samples_per_class*2\n",
    "# print('samples_per_class: ', samples_per_class)\n",
    "# print('samples_count: ', samples_count)\n",
    "\n",
    "# # Create a vertically stacked arra containing all sequences, then join labels\n",
    "# all_data = np.vstack((data_class_4, data_class_5))\n",
    "# print('all_data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "# print('all_labels.shape : ', all_labels.shape)\n",
    "\n",
    "# # Create a method for shuffling data\n",
    "# shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "# print(len(shuffle_indices))\n",
    "\n",
    "# # Assign a percentage of data for training and the rest for testing\n",
    "# train_size = math.floor(0.85*all_data.shape[0])\n",
    "# indices_train = shuffle_indices[0:train_size]\n",
    "# indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# # Define the data vs labels for each of the training ad test sets\n",
    "# X_train = all_data[indices_train,:]\n",
    "# Y_train = all_labels[indices_train]\n",
    "# X_test = all_data[indices_test,:]\n",
    "# Y_test = all_labels[indices_test]\n",
    "# print('X_train.shape : ', X_train.shape)\n",
    "# print('X_test.shape : ', X_test.shape)\n",
    "# print('Y_train.shape : ', Y_train.shape)\n",
    "# print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# # Define the input dimension from X_train.shape[1]\n",
    "# in_dim = X_train.shape[1]\n",
    "\n",
    "# # define the keras model\n",
    "# #model = Sequential()\n",
    "# #model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "# #model.add(Dense(64, activation='relu'))\n",
    "# #model.add(Dense(32, activation='relu'))\n",
    "# #model.add(Dense(2, activation='softmax'))\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # compile the keras model\n",
    "# #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Run the model\n",
    "# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.savefig('../../cryptic_1-4.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # C. albicans vs C. metapsilosis\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import plot_model\n",
    "# from PIL import Image\n",
    "# import random\n",
    "# import math\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# # First import the data by loading the file and extracting the correct section\n",
    "# data_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b3+b4_seqs.csv.npz')\n",
    "# data = data_npz['arr_0']\n",
    "# labels_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b3+b4_ids.csv.npz')\n",
    "# labels = labels_npz['arr_0']\n",
    "\n",
    "# # Print the shape of the resulting dataframes to visually verify\n",
    "# print(labels.shape)\n",
    "# print(data.shape)\n",
    "\n",
    "# # Separate the data into separate classes based on the labels\n",
    "# data_class_4 = data[(labels == 4)]\n",
    "# data_class_3 = data[(labels == 3)]\n",
    "# # Print an entry to visualise this\n",
    "# print(data_class_4[50])\n",
    "# print(data_class_3[50])\n",
    "\n",
    "# # Print the shape of these new arrays to visually verify\n",
    "# print('data_class_4.shape: ', data_class_4.shape)\n",
    "# print('data_class_3.shape: ', data_class_3.shape)\n",
    "\n",
    "# # Determine the total number of samples per class, and the total number of samples overall\n",
    "# samples_per_class = data_class_4.shape[0]\n",
    "# samples_count = samples_per_class*2\n",
    "# print('samples_per_class: ', samples_per_class)\n",
    "# print('samples_count: ', samples_count)\n",
    "\n",
    "# # Create a vertically stacked arra containing all sequences, then join labels\n",
    "# all_data = np.vstack((data_class_4, data_class_3))\n",
    "# print('all_data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "# print('all_labels.shape : ', all_labels.shape)\n",
    "\n",
    "# # Create a method for shuffling data\n",
    "# shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "# print(len(shuffle_indices))\n",
    "\n",
    "# # Assign a percentage of data for training and the rest for testing\n",
    "# train_size = math.floor(0.85*all_data.shape[0])\n",
    "# indices_train = shuffle_indices[0:train_size]\n",
    "# indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# # Define the data vs labels for each of the training ad test sets\n",
    "# X_train = all_data[indices_train,:]\n",
    "# Y_train = all_labels[indices_train]\n",
    "# X_test = all_data[indices_test,:]\n",
    "# Y_test = all_labels[indices_test]\n",
    "# print('X_train.shape : ', X_train.shape)\n",
    "# print('X_test.shape : ', X_test.shape)\n",
    "# print('Y_train.shape : ', Y_train.shape)\n",
    "# print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# # Define the input dimension from X_train.shape[1]\n",
    "# in_dim = X_train.shape[1]\n",
    "\n",
    "# # define the keras model\n",
    "# #model = Sequential()\n",
    "# #model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "# #model.add(Dense(64, activation='relu'))\n",
    "# #model.add(Dense(32, activation='relu'))\n",
    "# #model.add(Dense(2, activation='softmax'))\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # compile the keras model\n",
    "# #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Run the model\n",
    "# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Y. mexicana vs Y. scolyti\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import plot_model\n",
    "# from PIL import Image\n",
    "# import random\n",
    "# import math\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# # First import the data by loading the file and extracting the correct section\n",
    "# data_npz = np.load('../../analysis/arrays_test/20171207_FAH18654_b7+b8_seqs.csv.npz')\n",
    "# data = data_npz['arr_0']\n",
    "# labels_npz = np.load('../../analysis/arrays_test/20171207_FAH18654_b7+b8_ids.csv.npz')\n",
    "# labels = labels_npz['arr_0']\n",
    "\n",
    "# # Print the shape of the resulting dataframes to visually verify\n",
    "# print(labels.shape)\n",
    "# print(data.shape)\n",
    "\n",
    "# # Separate the data into separate classes based on the labels\n",
    "# data_class_7 = data[(labels == 7)]\n",
    "# data_class_8 = data[(labels == 8)]\n",
    "# # Print an entry to visualise this\n",
    "# print(data_class_7[50])\n",
    "# print(data_class_8[50])\n",
    "\n",
    "# # Print the shape of these new arrays to visually verify\n",
    "# print('data_class_7.shape: ', data_class_7.shape)\n",
    "# print('data_class_8.shape: ', data_class_8.shape)\n",
    "\n",
    "# # Determine the total number of samples per class, and the total number of samples overall\n",
    "# samples_per_class = data_class_7.shape[0]\n",
    "# samples_count = samples_per_class*2\n",
    "# print('samples_per_class: ', samples_per_class)\n",
    "# print('samples_count: ', samples_count)\n",
    "\n",
    "# # Create a vertically stacked arra containing all sequences, then join labels\n",
    "# all_data = np.vstack((data_class_7, data_class_8))\n",
    "# print('all_data.shape : ', all_data.shape)\n",
    "# all_labels = np.hstack( (np.zeros(samples_per_class), np.ones(samples_per_class)) )\n",
    "# print('all_labels.shape : ', all_labels.shape)\n",
    "\n",
    "# # Create a method for shuffling data\n",
    "# shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "# print(len(shuffle_indices))\n",
    "\n",
    "# # Assign a percentage of data for training and the rest for testing\n",
    "# train_size = math.floor(0.85*all_data.shape[0])\n",
    "# indices_train = shuffle_indices[0:train_size]\n",
    "# indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# # Define the data vs labels for each of the training ad test sets\n",
    "# X_train = all_data[indices_train,:]\n",
    "# Y_train = all_labels[indices_train]\n",
    "# X_test = all_data[indices_test,:]\n",
    "# Y_test = all_labels[indices_test]\n",
    "# print('X_train.shape : ', X_train.shape)\n",
    "# print('X_test.shape : ', X_test.shape)\n",
    "# print('Y_train.shape : ', Y_train.shape)\n",
    "# print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# # Define the input dimension from X_train.shape[1]\n",
    "# in_dim = X_train.shape[1]\n",
    "\n",
    "# # define the keras model\n",
    "# #model = Sequential()\n",
    "# #model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "# #model.add(Dense(64, activation='relu'))\n",
    "# #model.add(Dense(32, activation='relu'))\n",
    "# #model.add(Dense(2, activation='softmax'))\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # compile the keras model\n",
    "# #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Run the model\n",
    "# history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using padding 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z. tritici vs C. globuliformis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs4.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids4.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_2 = data[:samples_per_class,:]\n",
    "data_class_6 = data[samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_2[50])\n",
    "print(data_class_6[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for Z. tritici vs C. globuliformis (g.d. = 77.7%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using padding 0 with 1-4 for bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z. tritici vs C. globuliformis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs0.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids0.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_2 = data[:samples_per_class,:]\n",
    "data_class_6 = data[samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_2[50])\n",
    "print(data_class_6[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for Z. tritici vs C. globuliformis (padding 0, bases 1-4)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using padding 0\n",
    "###### Seems to work just fine!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z. tritici vs C. globuliformis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_2 = data[:samples_per_class,:]\n",
    "data_class_6 = data[samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_2[50])\n",
    "print(data_class_6[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for Z. tritic vs C. globuliformis (padding==A)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puccinia striiformis-tritici vs Zymoseptoria tritici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puccinia striiformis-tritici vs Zymoseptoria tritici\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b1+b2_seqs.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b1+b2_ids.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_1 = data[:samples_per_class,:]\n",
    "data_class_2 = data[samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_1[50])\n",
    "print(data_class_2[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_1.shape: ', data_class_1.shape)\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for P. striiformis-tritici vs Z. tritici (g.d. = 75.1%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspergillus niger vs Aspergillus flavus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspergillus niger vs Aspergillus flavus\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b7+20171207_FAH18654_b12_seqs.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b7+20171207_FAH18654_b12_ids.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_7 = data[:samples_per_class,:]\n",
    "data_class_12 = data[samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_7[50])\n",
    "print(data_class_12[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_7.shape: ', data_class_7.shape)\n",
    "print('data_class_12.shape: ', data_class_12.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for A. niger vs A. flavus (g.d. = 93.5%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candida albicans vs Candida metapsilosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candida albicans vs Candida metapsilosis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b3+b4_seqs.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b3+b4_ids.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_3 = data[:samples_per_class,:]\n",
    "data_class_4 = data[samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_3[50])\n",
    "print(data_class_4[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_3.shape: ', data_class_3.shape)\n",
    "print('data_class_4.shape: ', data_class_4.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for C. albicans vs C. metapsilosis (g.d. = 91.6%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candida metapsilosis vs Candida orthopsilosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candida metapsilosis vs Candida orthopsilosis\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5_seqs.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5_ids.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_4 = data[:samples_per_class,:]\n",
    "data_class_5 = data[samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_4[50])\n",
    "print(data_class_5[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_4.shape: ', data_class_4.shape)\n",
    "print('data_class_5.shape: ', data_class_5.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*2\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for C. albicans vs C. metapsilosis (g.d. = 97.7%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z. tritici vs C. globuliformis vs Candida albicans\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6+20180108_FAH18647_b3_seqs.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6+20180108_FAH18647_b3_ids.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_2 = data[:samples_per_class,:]\n",
    "data_class_6 = data[samples_per_class:2*samples_per_class,:]\n",
    "data_class_3 = data[2*samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_2[50])\n",
    "print(data_class_6[50])\n",
    "print(data_class_3[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_2.shape: ', data_class_2.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "print('data_class_3.shape: ', data_class_3.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*3\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for Z. tritici vs C. globuliformis vs C. albicans')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. albicans || metapsilosis || orthopsilosis || parapsilosis || unidentified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_labels_onehot.shape:  (45000, 3)\n",
      "data.shape: (45000, 3505)\n",
      "[1 2 3 ... 4 4 4]\n",
      "[1 3 1 ... 4 4 4]\n",
      "[4 1 3 ... 4 4 4]\n",
      "data_class_4.shape:  (15000, 3505)\n",
      "data_class_5.shape:  (15000, 3505)\n",
      "data_class_6.shape:  (15000, 3505)\n",
      "samples_per_class:  15000\n",
      "samples_count:  45000\n",
      "all_data.shape :  (45000, 3505)\n",
      "all_labels_onehot.shape :  (45000, 3)\n",
      "45000\n",
      "38250\n",
      "X_train.shape :  (38250, 3505)\n",
      "X_test.shape :  (6749, 3505)\n",
      "Y_train.shape :  (38250, 3)\n",
      "Y_test.shape :  (6749, 3)\n",
      "WARNING:tensorflow:From /home/tavish/anaconda3/envs/honours1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 38250 samples, validate on 6749 samples\n",
      "Epoch 1/100\n",
      "38250/38250 [==============================] - 8s 209us/step - loss: 1.2271 - accuracy: 0.3786 - val_loss: 1.0685 - val_accuracy: 0.4313\n",
      "Epoch 2/100\n",
      "38250/38250 [==============================] - 7s 187us/step - loss: 1.0455 - accuracy: 0.4585 - val_loss: 1.0420 - val_accuracy: 0.3947\n",
      "Epoch 3/100\n",
      "38250/38250 [==============================] - 7s 195us/step - loss: 0.9406 - accuracy: 0.5356 - val_loss: 0.9094 - val_accuracy: 0.5330\n",
      "Epoch 4/100\n",
      "38250/38250 [==============================] - 8s 202us/step - loss: 0.8752 - accuracy: 0.5729 - val_loss: 0.8775 - val_accuracy: 0.5547\n",
      "Epoch 5/100\n",
      "38250/38250 [==============================] - 7s 188us/step - loss: 0.8544 - accuracy: 0.5830 - val_loss: 0.8458 - val_accuracy: 0.5768\n",
      "Epoch 6/100\n",
      "38250/38250 [==============================] - 7s 186us/step - loss: 0.8022 - accuracy: 0.6174 - val_loss: 0.8015 - val_accuracy: 0.6219\n",
      "Epoch 7/100\n",
      "38250/38250 [==============================] - 7s 189us/step - loss: 0.7657 - accuracy: 0.6355 - val_loss: 0.7799 - val_accuracy: 0.6250\n",
      "Epoch 8/100\n",
      "38250/38250 [==============================] - 7s 181us/step - loss: 0.7497 - accuracy: 0.6367 - val_loss: 0.7818 - val_accuracy: 0.5982\n",
      "Epoch 9/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.7424 - accuracy: 0.6458 - val_loss: 0.8336 - val_accuracy: 0.5820\n",
      "Epoch 10/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.7335 - accuracy: 0.6546 - val_loss: 0.7603 - val_accuracy: 0.6365\n",
      "Epoch 11/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.7354 - accuracy: 0.6539 - val_loss: 0.8662 - val_accuracy: 0.5845\n",
      "Epoch 12/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.7286 - accuracy: 0.6503 - val_loss: 0.7679 - val_accuracy: 0.6351\n",
      "Epoch 13/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.7130 - accuracy: 0.6702 - val_loss: 0.7621 - val_accuracy: 0.6416\n",
      "Epoch 14/100\n",
      "38250/38250 [==============================] - 8s 214us/step - loss: 0.7087 - accuracy: 0.6676 - val_loss: 0.8829 - val_accuracy: 0.5559\n",
      "Epoch 15/100\n",
      "38250/38250 [==============================] - 8s 214us/step - loss: 0.7019 - accuracy: 0.6727 - val_loss: 0.8774 - val_accuracy: 0.5884\n",
      "Epoch 16/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.7157 - accuracy: 0.6683 - val_loss: 0.8536 - val_accuracy: 0.5956\n",
      "Epoch 17/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.6892 - accuracy: 0.6810 - val_loss: 0.9129 - val_accuracy: 0.5721\n",
      "Epoch 18/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.7073 - accuracy: 0.6699 - val_loss: 0.7943 - val_accuracy: 0.6213\n",
      "Epoch 19/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.7003 - accuracy: 0.6702 - val_loss: 0.7557 - val_accuracy: 0.6484\n",
      "Epoch 20/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6899 - accuracy: 0.6806 - val_loss: 0.8440 - val_accuracy: 0.6113\n",
      "Epoch 21/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6887 - accuracy: 0.6790 - val_loss: 0.7866 - val_accuracy: 0.6214\n",
      "Epoch 22/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.6870 - accuracy: 0.6807 - val_loss: 0.7649 - val_accuracy: 0.6410\n",
      "Epoch 23/100\n",
      "38250/38250 [==============================] - 8s 208us/step - loss: 0.6840 - accuracy: 0.6795 - val_loss: 0.7587 - val_accuracy: 0.6371\n",
      "Epoch 24/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6670 - accuracy: 0.6838 - val_loss: 0.7616 - val_accuracy: 0.6316\n",
      "Epoch 25/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6878 - accuracy: 0.6784 - val_loss: 0.7681 - val_accuracy: 0.6367\n",
      "Epoch 26/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.6998 - accuracy: 0.6764 - val_loss: 0.8364 - val_accuracy: 0.6088\n",
      "Epoch 27/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.6692 - accuracy: 0.6916 - val_loss: 0.7701 - val_accuracy: 0.6386\n",
      "Epoch 28/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6816 - accuracy: 0.6768 - val_loss: 0.7348 - val_accuracy: 0.6508\n",
      "Epoch 29/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.6669 - accuracy: 0.6863 - val_loss: 0.7369 - val_accuracy: 0.6509\n",
      "Epoch 30/100\n",
      "38250/38250 [==============================] - 8s 219us/step - loss: 0.6518 - accuracy: 0.6933 - val_loss: 0.7380 - val_accuracy: 0.6482\n",
      "Epoch 31/100\n",
      "38250/38250 [==============================] - 8s 213us/step - loss: 0.6882 - accuracy: 0.6754 - val_loss: 0.7937 - val_accuracy: 0.6164\n",
      "Epoch 32/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6529 - accuracy: 0.6915 - val_loss: 0.8167 - val_accuracy: 0.6102\n",
      "Epoch 33/100\n",
      "38250/38250 [==============================] - 8s 218us/step - loss: 0.6609 - accuracy: 0.6875 - val_loss: 0.9088 - val_accuracy: 0.5817\n",
      "Epoch 34/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6523 - accuracy: 0.6910 - val_loss: 0.8298 - val_accuracy: 0.6210\n",
      "Epoch 35/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6604 - accuracy: 0.6856 - val_loss: 0.8220 - val_accuracy: 0.6309\n",
      "Epoch 36/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.6404 - accuracy: 0.6960 - val_loss: 0.7382 - val_accuracy: 0.6425\n",
      "Epoch 37/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6555 - accuracy: 0.6878 - val_loss: 0.7286 - val_accuracy: 0.6429\n",
      "Epoch 38/100\n",
      "38250/38250 [==============================] - 8s 218us/step - loss: 0.6393 - accuracy: 0.6972 - val_loss: 0.7246 - val_accuracy: 0.6485\n",
      "Epoch 39/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6387 - accuracy: 0.6958 - val_loss: 0.7446 - val_accuracy: 0.6342\n",
      "Epoch 40/100\n",
      "38250/38250 [==============================] - 8s 219us/step - loss: 0.6383 - accuracy: 0.6935 - val_loss: 0.7622 - val_accuracy: 0.6519\n",
      "Epoch 41/100\n",
      "38250/38250 [==============================] - 8s 212us/step - loss: 0.6430 - accuracy: 0.6950 - val_loss: 0.7493 - val_accuracy: 0.6454\n",
      "Epoch 42/100\n",
      "38250/38250 [==============================] - 8s 219us/step - loss: 0.6299 - accuracy: 0.6992 - val_loss: 0.7266 - val_accuracy: 0.6561\n",
      "Epoch 43/100\n",
      "38250/38250 [==============================] - 8s 220us/step - loss: 0.6231 - accuracy: 0.7057 - val_loss: 0.8184 - val_accuracy: 0.6514\n",
      "Epoch 44/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.6102 - accuracy: 0.7114 - val_loss: 0.7277 - val_accuracy: 0.6644\n",
      "Epoch 45/100\n",
      "38250/38250 [==============================] - 8s 219us/step - loss: 0.5995 - accuracy: 0.7163 - val_loss: 0.7616 - val_accuracy: 0.6211\n",
      "Epoch 46/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.6214 - accuracy: 0.7036 - val_loss: 0.7317 - val_accuracy: 0.6490\n",
      "Epoch 47/100\n",
      "38250/38250 [==============================] - 8s 212us/step - loss: 0.6093 - accuracy: 0.7118 - val_loss: 0.7870 - val_accuracy: 0.6330\n",
      "Epoch 48/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.6307 - accuracy: 0.6980 - val_loss: 0.7602 - val_accuracy: 0.6481\n",
      "Epoch 49/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.5917 - accuracy: 0.7178 - val_loss: 0.7207 - val_accuracy: 0.6628\n",
      "Epoch 50/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.5952 - accuracy: 0.7170 - val_loss: 0.7329 - val_accuracy: 0.6631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.5993 - accuracy: 0.7147 - val_loss: 0.8208 - val_accuracy: 0.6351\n",
      "Epoch 52/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.5969 - accuracy: 0.7146 - val_loss: 0.7219 - val_accuracy: 0.6623\n",
      "Epoch 53/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.5961 - accuracy: 0.7189 - val_loss: 0.8948 - val_accuracy: 0.5879\n",
      "Epoch 54/100\n",
      "38250/38250 [==============================] - 8s 214us/step - loss: 0.5849 - accuracy: 0.7206 - val_loss: 0.8057 - val_accuracy: 0.6509\n",
      "Epoch 55/100\n",
      "38250/38250 [==============================] - 8s 208us/step - loss: 0.5727 - accuracy: 0.7274 - val_loss: 0.7210 - val_accuracy: 0.6554\n",
      "Epoch 56/100\n",
      "38250/38250 [==============================] - 8s 210us/step - loss: 0.5645 - accuracy: 0.7320 - val_loss: 0.7218 - val_accuracy: 0.6496\n",
      "Epoch 57/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.5722 - accuracy: 0.7278 - val_loss: 0.9707 - val_accuracy: 0.6460\n",
      "Epoch 58/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.5731 - accuracy: 0.7288 - val_loss: 0.7998 - val_accuracy: 0.6434\n",
      "Epoch 59/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.5746 - accuracy: 0.7277 - val_loss: 0.7255 - val_accuracy: 0.6665\n",
      "Epoch 60/100\n",
      "38250/38250 [==============================] - 8s 214us/step - loss: 0.5697 - accuracy: 0.7284 - val_loss: 0.7573 - val_accuracy: 0.6468\n",
      "Epoch 61/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.5662 - accuracy: 0.7277 - val_loss: 0.7628 - val_accuracy: 0.6512\n",
      "Epoch 62/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.5610 - accuracy: 0.7302 - val_loss: 0.7504 - val_accuracy: 0.6742\n",
      "Epoch 63/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.5687 - accuracy: 0.7267 - val_loss: 0.7392 - val_accuracy: 0.6586\n",
      "Epoch 64/100\n",
      "38250/38250 [==============================] - 8s 213us/step - loss: 0.5489 - accuracy: 0.7401 - val_loss: 0.7216 - val_accuracy: 0.6597\n",
      "Epoch 65/100\n",
      "38250/38250 [==============================] - 8s 212us/step - loss: 0.5542 - accuracy: 0.7387 - val_loss: 0.8467 - val_accuracy: 0.6137\n",
      "Epoch 66/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.5463 - accuracy: 0.7422 - val_loss: 0.7912 - val_accuracy: 0.6699\n",
      "Epoch 67/100\n",
      "38250/38250 [==============================] - 8s 214us/step - loss: 0.5587 - accuracy: 0.7314 - val_loss: 0.8375 - val_accuracy: 0.6183\n",
      "Epoch 68/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.5290 - accuracy: 0.7482 - val_loss: 0.8236 - val_accuracy: 0.6645\n",
      "Epoch 69/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.5413 - accuracy: 0.7416 - val_loss: 0.7785 - val_accuracy: 0.6653\n",
      "Epoch 70/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.5392 - accuracy: 0.7419 - val_loss: 0.8704 - val_accuracy: 0.6419\n",
      "Epoch 71/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.5369 - accuracy: 0.7416 - val_loss: 0.8817 - val_accuracy: 0.6445\n",
      "Epoch 72/100\n",
      "38250/38250 [==============================] - 8s 211us/step - loss: 0.5392 - accuracy: 0.7399 - val_loss: 0.7688 - val_accuracy: 0.6442\n",
      "Epoch 73/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.5337 - accuracy: 0.7439 - val_loss: 0.7387 - val_accuracy: 0.6690\n",
      "Epoch 74/100\n",
      "38250/38250 [==============================] - 8s 221us/step - loss: 0.5307 - accuracy: 0.7469 - val_loss: 0.9511 - val_accuracy: 0.6558\n",
      "Epoch 75/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.5171 - accuracy: 0.7533 - val_loss: 0.7477 - val_accuracy: 0.6690\n",
      "Epoch 76/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.5089 - accuracy: 0.7545 - val_loss: 0.7381 - val_accuracy: 0.6688\n",
      "Epoch 77/100\n",
      "38250/38250 [==============================] - 8s 214us/step - loss: 0.5229 - accuracy: 0.7482 - val_loss: 0.7532 - val_accuracy: 0.6669\n",
      "Epoch 78/100\n",
      "38250/38250 [==============================] - 8s 218us/step - loss: 0.5209 - accuracy: 0.7494 - val_loss: 0.7760 - val_accuracy: 0.6616\n",
      "Epoch 79/100\n",
      "38250/38250 [==============================] - 8s 219us/step - loss: 0.5295 - accuracy: 0.7456 - val_loss: 0.7495 - val_accuracy: 0.6561\n",
      "Epoch 80/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.5174 - accuracy: 0.7496 - val_loss: 0.8715 - val_accuracy: 0.6542\n",
      "Epoch 81/100\n",
      "38250/38250 [==============================] - 8s 219us/step - loss: 0.5042 - accuracy: 0.7551 - val_loss: 0.7586 - val_accuracy: 0.6635\n",
      "Epoch 82/100\n",
      "38250/38250 [==============================] - 8s 215us/step - loss: 0.5160 - accuracy: 0.7518 - val_loss: 0.7699 - val_accuracy: 0.6684\n",
      "Epoch 83/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.4964 - accuracy: 0.7588 - val_loss: 0.7916 - val_accuracy: 0.6675\n",
      "Epoch 84/100\n",
      "38250/38250 [==============================] - 8s 222us/step - loss: 0.5066 - accuracy: 0.7535 - val_loss: 0.8238 - val_accuracy: 0.6533\n",
      "Epoch 85/100\n",
      "38250/38250 [==============================] - 8s 219us/step - loss: 0.5053 - accuracy: 0.7563 - val_loss: 0.7630 - val_accuracy: 0.6376\n",
      "Epoch 86/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.5028 - accuracy: 0.7581 - val_loss: 0.8474 - val_accuracy: 0.6367\n",
      "Epoch 87/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.4875 - accuracy: 0.7645 - val_loss: 0.8032 - val_accuracy: 0.6712\n",
      "Epoch 88/100\n",
      "38250/38250 [==============================] - 8s 213us/step - loss: 0.4977 - accuracy: 0.7565 - val_loss: 0.7970 - val_accuracy: 0.6660\n",
      "Epoch 89/100\n",
      "38250/38250 [==============================] - 8s 220us/step - loss: 0.4816 - accuracy: 0.7664 - val_loss: 0.8213 - val_accuracy: 0.6634\n",
      "Epoch 90/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.4818 - accuracy: 0.7667 - val_loss: 0.7700 - val_accuracy: 0.6604\n",
      "Epoch 91/100\n",
      "38250/38250 [==============================] - 8s 218us/step - loss: 0.4909 - accuracy: 0.7623 - val_loss: 0.8822 - val_accuracy: 0.6743\n",
      "Epoch 92/100\n",
      "38250/38250 [==============================] - 8s 219us/step - loss: 0.4666 - accuracy: 0.7723 - val_loss: 0.8884 - val_accuracy: 0.6576\n",
      "Epoch 93/100\n",
      "38250/38250 [==============================] - 8s 217us/step - loss: 0.4730 - accuracy: 0.7714 - val_loss: 0.8367 - val_accuracy: 0.6721\n",
      "Epoch 94/100\n",
      "38250/38250 [==============================] - 8s 218us/step - loss: 0.4807 - accuracy: 0.7672 - val_loss: 0.7832 - val_accuracy: 0.6620\n",
      "Epoch 95/100\n",
      "38250/38250 [==============================] - 8s 214us/step - loss: 0.4642 - accuracy: 0.7757 - val_loss: 0.8723 - val_accuracy: 0.6081\n",
      "Epoch 96/100\n",
      "38250/38250 [==============================] - 8s 220us/step - loss: 0.4808 - accuracy: 0.7663 - val_loss: 0.8363 - val_accuracy: 0.6648\n",
      "Epoch 97/100\n",
      "38250/38250 [==============================] - 8s 218us/step - loss: 0.4633 - accuracy: 0.7741 - val_loss: 0.8911 - val_accuracy: 0.6530\n",
      "Epoch 98/100\n",
      "38250/38250 [==============================] - 8s 218us/step - loss: 0.4792 - accuracy: 0.7695 - val_loss: 0.9621 - val_accuracy: 0.6583\n",
      "Epoch 99/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.4565 - accuracy: 0.7770 - val_loss: 0.8234 - val_accuracy: 0.6641\n",
      "Epoch 100/100\n",
      "38250/38250 [==============================] - 8s 216us/step - loss: 0.4584 - accuracy: 0.7755 - val_loss: 0.8413 - val_accuracy: 0.6691\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cryptic Candida\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5+b6_seqs.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b4+b5+b6_ids.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "num_class = 3\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_4 = data[:samples_per_class,:]\n",
    "data_class_5 = data[samples_per_class:2*samples_per_class,:]\n",
    "data_class_6 = data[2*samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_4[50])\n",
    "print(data_class_5[50])\n",
    "print(data_class_6[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_4.shape: ', data_class_4.shape)\n",
    "print('data_class_5.shape: ', data_class_5.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*num_class\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# define the keras model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(128, input_dim=480, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_class, activation='softmax'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy for Cryptic Candida')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_labels_onehot.shape:  (45000, 3)\n",
      "data.shape: (45000, 4352)\n",
      "[1 1 3 ... 4 4 4]\n",
      "[3 1 4 ... 4 4 4]\n",
      "[2 1 3 ... 4 4 4]\n",
      "data_class_3.shape:  (15000, 4352)\n",
      "data_class_5.shape:  (15000, 4352)\n",
      "data_class_9.shape:  (15000, 4352)\n",
      "samples_per_class:  15000\n",
      "samples_count:  45000\n",
      "all_data.shape :  (45000, 4352)\n",
      "all_labels_onehot.shape :  (45000, 3)\n",
      "45000\n",
      "38250\n",
      "X_train.shape :  (38250, 4352)\n",
      "X_test.shape :  (6749, 4352)\n",
      "Y_train.shape :  (38250, 3)\n",
      "Y_test.shape :  (6749, 3)\n",
      "Train on 38250 samples, validate on 6749 samples\n",
      "Epoch 1/100\n",
      "38250/38250 [==============================] - 3s 67us/step - loss: 1.1159 - accuracy: 0.3787 - val_loss: 1.0809 - val_accuracy: 0.3246\n",
      "Epoch 2/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 1.0462 - accuracy: 0.4296 - val_loss: 1.0221 - val_accuracy: 0.4094\n",
      "Epoch 3/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.9725 - accuracy: 0.5093 - val_loss: 0.8976 - val_accuracy: 0.5690\n",
      "Epoch 4/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.8764 - accuracy: 0.5683 - val_loss: 0.8172 - val_accuracy: 0.6079\n",
      "Epoch 5/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.8121 - accuracy: 0.6124 - val_loss: 0.8363 - val_accuracy: 0.5838\n",
      "Epoch 6/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7903 - accuracy: 0.6303 - val_loss: 0.8257 - val_accuracy: 0.5897\n",
      "Epoch 7/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7756 - accuracy: 0.6398 - val_loss: 0.7681 - val_accuracy: 0.6328\n",
      "Epoch 8/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.7548 - accuracy: 0.6524 - val_loss: 0.8387 - val_accuracy: 0.5863\n",
      "Epoch 9/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7820 - accuracy: 0.6224 - val_loss: 0.9424 - val_accuracy: 0.4538\n",
      "Epoch 10/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.8630 - accuracy: 0.5390 - val_loss: 0.8209 - val_accuracy: 0.6502\n",
      "Epoch 11/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7584 - accuracy: 0.6288 - val_loss: 0.7446 - val_accuracy: 0.6359\n",
      "Epoch 12/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.7508 - accuracy: 0.6377 - val_loss: 0.8439 - val_accuracy: 0.6139\n",
      "Epoch 13/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7735 - accuracy: 0.6443 - val_loss: 0.7557 - val_accuracy: 0.6439\n",
      "Epoch 14/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7272 - accuracy: 0.6490 - val_loss: 0.7422 - val_accuracy: 0.6477\n",
      "Epoch 15/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.7154 - accuracy: 0.6555 - val_loss: 0.7340 - val_accuracy: 0.6493\n",
      "Epoch 16/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7206 - accuracy: 0.6673 - val_loss: 0.7302 - val_accuracy: 0.6699\n",
      "Epoch 17/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7259 - accuracy: 0.6659 - val_loss: 0.7407 - val_accuracy: 0.6500\n",
      "Epoch 18/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7068 - accuracy: 0.6757 - val_loss: 0.7294 - val_accuracy: 0.6623\n",
      "Epoch 19/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.7071 - accuracy: 0.6743 - val_loss: 0.7846 - val_accuracy: 0.6494\n",
      "Epoch 20/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.7035 - accuracy: 0.6765 - val_loss: 0.7296 - val_accuracy: 0.6644\n",
      "Epoch 21/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7211 - accuracy: 0.6628 - val_loss: 0.7606 - val_accuracy: 0.6543\n",
      "Epoch 22/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.7070 - accuracy: 0.6741 - val_loss: 0.7211 - val_accuracy: 0.6694\n",
      "Epoch 23/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6952 - accuracy: 0.6764 - val_loss: 0.7097 - val_accuracy: 0.6657\n",
      "Epoch 24/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6848 - accuracy: 0.6754 - val_loss: 0.7005 - val_accuracy: 0.6696\n",
      "Epoch 25/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.6760 - accuracy: 0.6799 - val_loss: 0.7081 - val_accuracy: 0.6645\n",
      "Epoch 26/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6750 - accuracy: 0.6763 - val_loss: 0.7230 - val_accuracy: 0.6496\n",
      "Epoch 27/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6812 - accuracy: 0.6767 - val_loss: 0.7255 - val_accuracy: 0.6462\n",
      "Epoch 28/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6674 - accuracy: 0.6837 - val_loss: 0.7044 - val_accuracy: 0.6650\n",
      "Epoch 29/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6632 - accuracy: 0.6863 - val_loss: 0.6943 - val_accuracy: 0.6688\n",
      "Epoch 30/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6687 - accuracy: 0.6815 - val_loss: 0.7160 - val_accuracy: 0.6724\n",
      "Epoch 31/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6657 - accuracy: 0.6835 - val_loss: 0.6982 - val_accuracy: 0.6771\n",
      "Epoch 32/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.6747 - accuracy: 0.6797 - val_loss: 0.7117 - val_accuracy: 0.6515\n",
      "Epoch 33/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6636 - accuracy: 0.6837 - val_loss: 0.7206 - val_accuracy: 0.6613\n",
      "Epoch 34/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.6525 - accuracy: 0.6909 - val_loss: 0.7447 - val_accuracy: 0.6437\n",
      "Epoch 35/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.6519 - accuracy: 0.6907 - val_loss: 0.6893 - val_accuracy: 0.6699\n",
      "Epoch 36/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.6598 - accuracy: 0.6872 - val_loss: 0.7117 - val_accuracy: 0.6749\n",
      "Epoch 37/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6568 - accuracy: 0.6893 - val_loss: 0.7113 - val_accuracy: 0.6740\n",
      "Epoch 38/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6445 - accuracy: 0.6946 - val_loss: 0.6990 - val_accuracy: 0.6614\n",
      "Epoch 39/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.6607 - accuracy: 0.6865 - val_loss: 0.7223 - val_accuracy: 0.6641\n",
      "Epoch 40/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6416 - accuracy: 0.6967 - val_loss: 0.6899 - val_accuracy: 0.6792\n",
      "Epoch 41/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6318 - accuracy: 0.6996 - val_loss: 0.7024 - val_accuracy: 0.6742\n",
      "Epoch 42/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6415 - accuracy: 0.6907 - val_loss: 0.6869 - val_accuracy: 0.6725\n",
      "Epoch 43/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.6237 - accuracy: 0.7033 - val_loss: 0.6995 - val_accuracy: 0.6706\n",
      "Epoch 44/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6231 - accuracy: 0.7049 - val_loss: 0.6858 - val_accuracy: 0.6752\n",
      "Epoch 45/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.6332 - accuracy: 0.6975 - val_loss: 0.7000 - val_accuracy: 0.6626\n",
      "Epoch 46/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6258 - accuracy: 0.7003 - val_loss: 0.6890 - val_accuracy: 0.6711\n",
      "Epoch 47/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6138 - accuracy: 0.7091 - val_loss: 0.6915 - val_accuracy: 0.6807\n",
      "Epoch 48/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.6118 - accuracy: 0.7103 - val_loss: 0.6984 - val_accuracy: 0.6725\n",
      "Epoch 49/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.6176 - accuracy: 0.7052 - val_loss: 0.7274 - val_accuracy: 0.6583\n",
      "Epoch 50/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.5871 - accuracy: 0.7196 - val_loss: 0.7314 - val_accuracy: 0.6656\n",
      "Epoch 51/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5990 - accuracy: 0.7166 - val_loss: 0.6894 - val_accuracy: 0.6725\n",
      "Epoch 52/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.5912 - accuracy: 0.7185 - val_loss: 0.7459 - val_accuracy: 0.6743\n",
      "Epoch 53/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5971 - accuracy: 0.7167 - val_loss: 0.7664 - val_accuracy: 0.6451\n",
      "Epoch 54/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5850 - accuracy: 0.7227 - val_loss: 0.7154 - val_accuracy: 0.6545\n",
      "Epoch 55/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5914 - accuracy: 0.7175 - val_loss: 0.6707 - val_accuracy: 0.6863\n",
      "Epoch 56/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5762 - accuracy: 0.7273 - val_loss: 0.8370 - val_accuracy: 0.6168\n",
      "Epoch 57/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.6150 - accuracy: 0.7087 - val_loss: 0.7279 - val_accuracy: 0.6797\n",
      "Epoch 58/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5610 - accuracy: 0.7354 - val_loss: 0.7494 - val_accuracy: 0.6660\n",
      "Epoch 59/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5590 - accuracy: 0.7362 - val_loss: 0.6707 - val_accuracy: 0.6887\n",
      "Epoch 60/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.5797 - accuracy: 0.7254 - val_loss: 0.6994 - val_accuracy: 0.6791\n",
      "Epoch 61/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.5542 - accuracy: 0.7370 - val_loss: 0.7560 - val_accuracy: 0.6631\n",
      "Epoch 62/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5397 - accuracy: 0.7461 - val_loss: 0.6842 - val_accuracy: 0.6780\n",
      "Epoch 63/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5620 - accuracy: 0.7336 - val_loss: 0.6987 - val_accuracy: 0.6883\n",
      "Epoch 64/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5513 - accuracy: 0.7385 - val_loss: 0.7499 - val_accuracy: 0.6927\n",
      "Epoch 65/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5406 - accuracy: 0.7439 - val_loss: 0.6676 - val_accuracy: 0.6887\n",
      "Epoch 66/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.5417 - accuracy: 0.7446 - val_loss: 0.7479 - val_accuracy: 0.6820\n",
      "Epoch 67/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5390 - accuracy: 0.7460 - val_loss: 0.6972 - val_accuracy: 0.6928\n",
      "Epoch 68/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5317 - accuracy: 0.7513 - val_loss: 0.7092 - val_accuracy: 0.6811\n",
      "Epoch 69/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.5282 - accuracy: 0.7515 - val_loss: 0.7743 - val_accuracy: 0.6899\n",
      "Epoch 70/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5210 - accuracy: 0.7537 - val_loss: 0.6895 - val_accuracy: 0.6914\n",
      "Epoch 71/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.5196 - accuracy: 0.7563 - val_loss: 0.6739 - val_accuracy: 0.6906\n",
      "Epoch 72/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.5120 - accuracy: 0.7576 - val_loss: 0.6890 - val_accuracy: 0.7028\n",
      "Epoch 73/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.5148 - accuracy: 0.7593 - val_loss: 0.7195 - val_accuracy: 0.6964\n",
      "Epoch 74/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5085 - accuracy: 0.7601 - val_loss: 0.6978 - val_accuracy: 0.6883\n",
      "Epoch 75/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5266 - accuracy: 0.7516 - val_loss: 0.6579 - val_accuracy: 0.6971\n",
      "Epoch 76/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5024 - accuracy: 0.7634 - val_loss: 0.6987 - val_accuracy: 0.7069\n",
      "Epoch 77/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4947 - accuracy: 0.7700 - val_loss: 0.8485 - val_accuracy: 0.6674\n",
      "Epoch 78/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5064 - accuracy: 0.7626 - val_loss: 0.7362 - val_accuracy: 0.6936\n",
      "Epoch 79/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.5069 - accuracy: 0.7633 - val_loss: 0.7630 - val_accuracy: 0.6942\n",
      "Epoch 80/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.5044 - accuracy: 0.7666 - val_loss: 0.8114 - val_accuracy: 0.6868\n",
      "Epoch 81/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4907 - accuracy: 0.7718 - val_loss: 0.7058 - val_accuracy: 0.6918\n",
      "Epoch 82/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4925 - accuracy: 0.7732 - val_loss: 0.7471 - val_accuracy: 0.6585\n",
      "Epoch 83/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.4911 - accuracy: 0.7751 - val_loss: 0.7250 - val_accuracy: 0.6859\n",
      "Epoch 84/100\n",
      "38250/38250 [==============================] - 2s 60us/step - loss: 0.4885 - accuracy: 0.7751 - val_loss: 0.6840 - val_accuracy: 0.7043\n",
      "Epoch 85/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.4706 - accuracy: 0.7826 - val_loss: 0.6934 - val_accuracy: 0.7035\n",
      "Epoch 86/100\n",
      "38250/38250 [==============================] - 2s 60us/step - loss: 0.4722 - accuracy: 0.7832 - val_loss: 0.7275 - val_accuracy: 0.7003\n",
      "Epoch 87/100\n",
      "38250/38250 [==============================] - 2s 60us/step - loss: 0.4703 - accuracy: 0.7847 - val_loss: 0.7419 - val_accuracy: 0.7045\n",
      "Epoch 88/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4708 - accuracy: 0.7851 - val_loss: 0.7108 - val_accuracy: 0.6939\n",
      "Epoch 89/100\n",
      "38250/38250 [==============================] - 2s 60us/step - loss: 0.4629 - accuracy: 0.7905 - val_loss: 0.6982 - val_accuracy: 0.7057\n",
      "Epoch 90/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4529 - accuracy: 0.7936 - val_loss: 0.7127 - val_accuracy: 0.7155\n",
      "Epoch 91/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.4634 - accuracy: 0.7891 - val_loss: 0.8166 - val_accuracy: 0.6877\n",
      "Epoch 92/100\n",
      "38250/38250 [==============================] - 2s 63us/step - loss: 0.4685 - accuracy: 0.7864 - val_loss: 0.6638 - val_accuracy: 0.7087\n",
      "Epoch 93/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4447 - accuracy: 0.7985 - val_loss: 0.6805 - val_accuracy: 0.7163\n",
      "Epoch 94/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4345 - accuracy: 0.8044 - val_loss: 0.7620 - val_accuracy: 0.7077\n",
      "Epoch 95/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4341 - accuracy: 0.8044 - val_loss: 0.8246 - val_accuracy: 0.6819\n",
      "Epoch 96/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4317 - accuracy: 0.8070 - val_loss: 0.7491 - val_accuracy: 0.7186\n",
      "Epoch 97/100\n",
      "38250/38250 [==============================] - 2s 62us/step - loss: 0.4192 - accuracy: 0.8153 - val_loss: 0.6947 - val_accuracy: 0.7130\n",
      "Epoch 98/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.4331 - accuracy: 0.8073 - val_loss: 0.6898 - val_accuracy: 0.7115\n",
      "Epoch 99/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.4113 - accuracy: 0.8158 - val_loss: 0.7084 - val_accuracy: 0.7081\n",
      "Epoch 100/100\n",
      "38250/38250 [==============================] - 2s 61us/step - loss: 0.4323 - accuracy: 0.8053 - val_loss: 0.7261 - val_accuracy: 0.7037\n",
      "(6749,)\n",
      "[2 0 1 ... 1 1 2]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-883ab1885655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0myhat_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myhat_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0myhat_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myhat_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# accuracy: (tp + tn) / (p + n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# Candida albicans vs Candida orthopsilosis vs Candida unidentified\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b3+b5+b9_seqs.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b3+b5+b9_ids.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "num_class = 3\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_3 = data[:samples_per_class,:]\n",
    "data_class_5 = data[samples_per_class:2*samples_per_class,:]\n",
    "data_class_9 = data[2*samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_3[50])\n",
    "print(data_class_5[50])\n",
    "print(data_class_9[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_3.shape: ', data_class_3.shape)\n",
    "print('data_class_5.shape: ', data_class_5.shape)\n",
    "print('data_class_9.shape: ', data_class_9.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*num_class\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "def get_model(X_train, Y_train, num_class):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "    return model\n",
    "\n",
    "model = get_model(X_train, Y_train, num_class)\n",
    "\n",
    "yhat_probs = model.predict(X_test, verbose=0)\n",
    "yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "print(yhat_classes.shape)\n",
    "print(yhat_classes)\n",
    "\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(Y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(testy, yhat_classes)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_labels_onehot.shape:  (75000, 5)\n",
      "data.shape: (75000, 4352)\n",
      "[3 1 2 ... 4 4 4]\n",
      "[2 3 4 ... 4 4 4]\n",
      "[3 3 4 ... 4 4 4]\n",
      "[3 4 4 ... 4 4 4]\n",
      "[3 1 2 ... 4 4 4]\n",
      "data_class_3.shape:  (15000, 4352)\n",
      "data_class_4.shape:  (15000, 4352)\n",
      "data_class_5.shape:  (15000, 4352)\n",
      "data_class_6.shape:  (15000, 4352)\n",
      "data_class_9.shape:  (15000, 4352)\n",
      "samples_per_class:  15000\n",
      "samples_count:  75000\n",
      "all_data.shape :  (75000, 4352)\n",
      "all_labels_onehot.shape :  (75000, 5)\n",
      "75000\n",
      "63750\n",
      "X_train.shape :  (63750, 4352)\n",
      "X_test.shape :  (11249, 4352)\n",
      "Y_train.shape :  (63750, 5)\n",
      "Y_test.shape :  (11249, 5)\n",
      "WARNING:tensorflow:From /home/tavish/anaconda3/envs/honours1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 63750 samples, validate on 11249 samples\n",
      "Epoch 1/100\n",
      "63750/63750 [==============================] - 4s 66us/step - loss: 1.6339 - accuracy: 0.1985 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 2/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1974 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 3/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1988 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 4/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1985 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 5/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2003 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 6/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1995 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 7/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1995 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 8/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1973 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 9/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2004 - val_loss: 1.6095 - val_accuracy: 0.1992\n",
      "Epoch 10/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1970 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 11/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1996 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 12/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1980 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 13/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2003 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 14/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1973 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 15/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1981 - val_loss: 1.6095 - val_accuracy: 0.1992\n",
      "Epoch 16/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1977 - val_loss: 1.6094 - val_accuracy: 0.1954\n",
      "Epoch 17/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1995 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 18/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2006 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 19/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1966 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 20/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1997 - val_loss: 1.6094 - val_accuracy: 0.1982\n",
      "Epoch 21/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1978 - val_loss: 1.6096 - val_accuracy: 0.1982\n",
      "Epoch 22/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1983 - val_loss: 1.6096 - val_accuracy: 0.1995\n",
      "Epoch 23/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1991 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 24/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1994 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 25/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6097 - val_accuracy: 0.1954\n",
      "Epoch 26/100\n",
      "63750/63750 [==============================] - 4s 58us/step - loss: 1.6095 - accuracy: 0.1992 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 27/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1991 - val_loss: 1.6094 - val_accuracy: 0.1954\n",
      "Epoch 28/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1987 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 29/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1985 - val_loss: 1.6096 - val_accuracy: 0.1995\n",
      "Epoch 30/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2078\n",
      "Epoch 31/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2016 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 32/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2001 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 33/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1993 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 34/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1985 - val_loss: 1.6094 - val_accuracy: 0.1992\n",
      "Epoch 35/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1983 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 36/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1985 - val_loss: 1.6096 - val_accuracy: 0.1982\n",
      "Epoch 37/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1975 - val_loss: 1.6098 - val_accuracy: 0.1954\n",
      "Epoch 38/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1982 - val_loss: 1.6095 - val_accuracy: 0.1992\n",
      "Epoch 39/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2004 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 40/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1976 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 41/100\n",
      "63750/63750 [==============================] - 4s 58us/step - loss: 1.6095 - accuracy: 0.1989 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 42/100\n",
      "63750/63750 [==============================] - 4s 58us/step - loss: 1.6095 - accuracy: 0.1984 - val_loss: 1.6097 - val_accuracy: 0.1954\n",
      "Epoch 43/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1999 - val_loss: 1.6097 - val_accuracy: 0.1982\n",
      "Epoch 44/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1977 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 45/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2003 - val_loss: 1.6096 - val_accuracy: 0.1982\n",
      "Epoch 46/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1989 - val_loss: 1.6096 - val_accuracy: 0.1995\n",
      "Epoch 47/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1973 - val_loss: 1.6094 - val_accuracy: 0.1954\n",
      "Epoch 48/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 49/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1986 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 50/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1985 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 51/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1988 - val_loss: 1.6097 - val_accuracy: 0.1982\n",
      "Epoch 52/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1996 - val_loss: 1.6094 - val_accuracy: 0.2078\n",
      "Epoch 53/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2003 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 54/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1974 - val_loss: 1.6095 - val_accuracy: 0.1992\n",
      "Epoch 55/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1996 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 56/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1996 - val_loss: 1.6095 - val_accuracy: 0.1992\n",
      "Epoch 57/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1990 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 58/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2021 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 59/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1992 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 60/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2007 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 61/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1988 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 62/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1989 - val_loss: 1.6094 - val_accuracy: 0.1995\n",
      "Epoch 63/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1983 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 64/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1989 - val_loss: 1.6097 - val_accuracy: 0.1954\n",
      "Epoch 65/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1968 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 66/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1963 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 67/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1993 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 68/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1997 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 69/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2011 - val_loss: 1.6096 - val_accuracy: 0.1982\n",
      "Epoch 70/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1986 - val_loss: 1.6094 - val_accuracy: 0.1995\n",
      "Epoch 71/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1978 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 72/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2010 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 73/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1986 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 74/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1995 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 75/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1976 - val_loss: 1.6095 - val_accuracy: 0.1992\n",
      "Epoch 76/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1971 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 77/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2003 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 78/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1999 - val_loss: 1.6097 - val_accuracy: 0.1992\n",
      "Epoch 79/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1981 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 80/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1994 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 81/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1984 - val_loss: 1.6094 - val_accuracy: 0.2078\n",
      "Epoch 82/100\n",
      "63750/63750 [==============================] - 4s 58us/step - loss: 1.6095 - accuracy: 0.1991 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 83/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1989 - val_loss: 1.6095 - val_accuracy: 0.1992\n",
      "Epoch 84/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1996 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 85/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1986 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 86/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1976 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 87/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1990 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 88/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.1991 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 89/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1974 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 90/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1979 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 91/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1963 - val_loss: 1.6095 - val_accuracy: 0.1982\n",
      "Epoch 92/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1986 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 93/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1992 - val_loss: 1.6096 - val_accuracy: 0.1995\n",
      "Epoch 94/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1963 - val_loss: 1.6095 - val_accuracy: 0.1954\n",
      "Epoch 95/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1981 - val_loss: 1.6095 - val_accuracy: 0.1995\n",
      "Epoch 96/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2008 - val_loss: 1.6097 - val_accuracy: 0.1992\n",
      "Epoch 97/100\n",
      "63750/63750 [==============================] - 4s 60us/step - loss: 1.6095 - accuracy: 0.2016 - val_loss: 1.6096 - val_accuracy: 0.1995\n",
      "Epoch 98/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1966 - val_loss: 1.6096 - val_accuracy: 0.1954\n",
      "Epoch 99/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.2021 - val_loss: 1.6097 - val_accuracy: 0.1954\n",
      "Epoch 100/100\n",
      "63750/63750 [==============================] - 4s 59us/step - loss: 1.6095 - accuracy: 0.1993 - val_loss: 1.6096 - val_accuracy: 0.1954\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3e7dd934162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0myhat_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myhat_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0myhat_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myhat_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# accuracy: (tp + tn) / (p + n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# C. albicans || metapsilosis || orthopsilosis || parapsilosis || unidentified\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "def get_model(X_train, Y_train, num_class):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=in_dim))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "    return model\n",
    "\n",
    "# First import the data by loading the file and extracting the correct section\n",
    "data_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b3+b4+b5+b6+b9_seqs.csv.npz', allow_pickle=True)\n",
    "labels_npz = np.load('../../analysis/arrays_test/20180108_FAH18647_b3+b4+b5+b6+b9_ids.csv.npz', allow_pickle=True)\n",
    "\n",
    "data = data_npz['arr_0']\n",
    "all_labels_onehot = labels_npz['arr_0']\n",
    "\n",
    "samples_per_class = 15000\n",
    "\n",
    "num_class = 5\n",
    "\n",
    "# Print the shape of the resulting dataframes to visually verify\n",
    "print('all_labels_onehot.shape: ', all_labels_onehot.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "# Separate the data into separate classes based on the labels\n",
    "data_class_3 = data[:samples_per_class,:]\n",
    "data_class_4 = data[samples_per_class:2*samples_per_class,:]\n",
    "data_class_5 = data[2*samples_per_class:3*samples_per_class,:]\n",
    "data_class_6 = data[3*samples_per_class:4*samples_per_class,:]\n",
    "data_class_9 = data[4*samples_per_class:,:]\n",
    "\n",
    "# Print an entry to visualise this\n",
    "print(data_class_3[50])\n",
    "print(data_class_4[50])\n",
    "print(data_class_5[50])\n",
    "print(data_class_6[50])\n",
    "print(data_class_9[50])\n",
    "\n",
    "# Print the shape of these new arrays to visually verify\n",
    "print('data_class_3.shape: ', data_class_3.shape)\n",
    "print('data_class_4.shape: ', data_class_4.shape)\n",
    "print('data_class_5.shape: ', data_class_5.shape)\n",
    "print('data_class_6.shape: ', data_class_6.shape)\n",
    "print('data_class_9.shape: ', data_class_9.shape)\n",
    "\n",
    "# Determine the total number of samples per class, and the total number of samples overall\n",
    "samples_count = samples_per_class*num_class\n",
    "print('samples_per_class: ', samples_per_class)\n",
    "print('samples_count: ', samples_count)\n",
    "\n",
    "# Create a vertically stacked arra containing all sequences, then join labels\n",
    "all_data = data_npz['arr_0']\n",
    "print('all_data.shape : ', all_data.shape)\n",
    "print('all_labels_onehot.shape : ', all_labels_onehot.shape)\n",
    "\n",
    "# Create a method for shuffling data\n",
    "shuffle_indices = random.sample(range(0, samples_count), samples_count)\n",
    "print(len(shuffle_indices))\n",
    "\n",
    "# Assign a percentage of data for training and the rest for testing\n",
    "train_size = math.floor(0.85*data.shape[0])\n",
    "print(train_size)\n",
    "indices_train = shuffle_indices[0:train_size]\n",
    "indices_test = shuffle_indices[train_size+1:samples_count]\n",
    "\n",
    "# Define the data vs labels for each of the training and test sets\n",
    "X_train = data[indices_train,:]\n",
    "Y_train = all_labels_onehot[indices_train]\n",
    "\n",
    "X_test = data[indices_test,:]\n",
    "Y_test = all_labels_onehot[indices_test]\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "\n",
    "# Define the input dimension from X_train.shape[1]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "model = get_model(X_train, Y_train, num_class)\n",
    "\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy for Candida species')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "yhat_probs = model.predict(X_test, verbose=0)\n",
    "yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(Y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(testy, yhat_classes)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_fun(a, b, c):\n",
    "    d = a + b\n",
    "    e = a*b\n",
    "    f = a + b + c\n",
    "    return d, e, f\n",
    "\n",
    "test_fun(1,2,3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
